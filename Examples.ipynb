{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Based Tokenizer\n",
    "\n",
    "- Split on some delimiter\n",
    "- Most basically: split on whitespace.\n",
    "- Split on punctuation and more tokens for more advanced behaviour. \n",
    "\n",
    "Depending on your delimiter, you will get different tokens with different meanings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', \"aren't\", 'special', 'enough', 'to', 'break', 'the', 'rules.']\n"
     ]
    }
   ],
   "source": [
    "python_zen = \"Special cases aren't special enough to break the rules.\"\n",
    "print(python_zen.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rules.` will be different from `rules`, resulting in many tokens that represent the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**We should probably split on punctuation as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', 'aren', \"'\", 't', 'special', 'enough', 'to', 'break', 'the', 'rules', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize\n",
    "print(wordpunct_tokenize(python_zen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', 'are', \"n't\", 'special', 'enough', 'to', 'break', 'the', 'rules', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(python_zen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- `are` and `aren't` are still fairly simple, since `n't` is typically appended to negate something. \n",
    "- Having a rule to handle `n't` works well enough. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, this doesn't hold for other combined words. For example:\n",
    "\n",
    "- `token` \n",
    "- `tokens`\n",
    "- `tokenizer`\n",
    "- `tokenization` \n",
    "\n",
    "all are unique tokens and get corresponding unique IDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result in **a lot** of tokens!\n",
    "\n",
    "A common way of dealing with this was to limit the vocabulary size. Typically, the $n$ most frequent words will be allowed in the vocab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Character Based Tokenizers\n",
    "These tokenizers are conceptually very simple. Each character is a token.\n",
    "\n",
    "- `Small vocabulary`. For ASCII, 256. With unicode, this is 1.1M characters.\n",
    "- No out of vocabulary (OOV) words/characters.\n",
    "- Lose a lot of meaninful information\n",
    "- Models typically have `max input lengths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'a', 't']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('Cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mapping to a vocabulary\n",
    "The vocabulary of a text will be really simple, with one index mapping to a letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'C', 1: 'o', 2: 'm', 3: 'p', 4: 'l'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(enumerate(list(python_zen)[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Character based tokenization of Chinese characters\n",
    "*Disclaimer: I do not speak a word of chinese*\n",
    "\n",
    "- Characters have meaning by themselves\n",
    "- But can change by combining characters\n",
    "\n",
    "For example:\n",
    "\n",
    "[馬來西亞 means Malaysia, but taken separately they mean \"Horse come to Western Asia\".](https://medium.com/@jjsham/nlp-tokenizing-chinese-phases-3302da4336bf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Some good reads that use character based tokenization:\n",
    "1. [Neural Machine Translation in Linear Time. Kalchbrenner et al. 2017](https://arxiv.org/pdf/1610.10099.pdf)\n",
    "2. [Fully Character-Level Neural Machine Translation without Explicit Segmentation - Lee et al. 2017.](https://aclanthology.org/Q17-1026.pdf)\n",
    "3. [Learning to Generate Reviews and Discovering Sentiment - Radford et al. 2017.](https://arxiv.org/pdf/1704.01444.pdf)\n",
    "\n",
    "From these papers, you can already see that these tokenizers were mainly used a while back, around 2017. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "c38f211982bd679d5ca5d79a71d7517c654455bae4af5c30a3c557803a2cce89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
