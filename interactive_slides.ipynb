{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Based Tokenizer\n",
    "\n",
    "- Split on some delimiter\n",
    "- Most basically: split on whitespace.\n",
    "- Split on punctuation and more tokens for more advanced behaviour. \n",
    "\n",
    "Depending on your delimiter, you will get different tokens with different meanings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', \"aren't\", 'special', 'enough', 'to', 'break', 'the', 'rules.']\n"
     ]
    }
   ],
   "source": [
    "python_zen = \"Special cases aren't special enough to break the rules.\"\n",
    "tokens = python_zen.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now create IDs from this as well. For this, we create a mapping between our tokens and IDs, which we call the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Special': 0, 'cases': 1, \"aren't\": 2, 'special': 3, 'enough': 4, 'to': 5, 'break': 6, 'the': 7, 'rules.': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab = {y: x for x, y in enumerate(tokens)}\n",
    "print(vocab)  # We can show the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['special']  # We can map tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!-- ![obama](images/obama.gif) -->\n",
    "<img src=\"images/obama.gif\" style=\"margin-left:auto;margin-right:auto;height:500px%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a closer look at our tokens\n",
    "\n",
    "```python\n",
    "['Special', 'cases', \"aren't\", 'special', 'enough', 'to', 'break', 'the', 'rules.']\n",
    "```\n",
    "\n",
    "We see `rules.` has a dot in its token. So the token `rules` will be different from `rules.`. Maybe that's not super optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**We should probably split on punctuation as well**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting on punctuation and more\n",
    "We can use `NLTK` (Natural Language Toolkit), a super useful NLP package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', 'aren', \"'\", 't', 'special', 'enough', 'to', 'break', 'the', 'rules', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize  # Simple RegEx based tokenizer\n",
    "print(wordpunct_tokenize(python_zen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Even more smartly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/baukebrenninkmeijer/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', 'are', \"n't\", 'special', 'enough', 'to', 'break', 'the', 'rules', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  # Requires a download. Uses improved TreebankWordTokenizer + tricks.\n",
    "print(word_tokenize(python_zen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `are` and `aren't` are still fairly simple, since `n't` is typically appended to negate something. Part of standard contractions in English. \n",
    "- Having a rule to handle `n't` works well enough. (e.g., \n",
    "```python\n",
    ".replace(\"n't\", \" n't\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, this doesn't hold for other combined words. For example:\n",
    "\n",
    "- `token` \n",
    "- `tokens`\n",
    "- `tokenizer`\n",
    "- `tokenization` \n",
    "\n",
    "all are unique tokens and get corresponding unique IDs. \n",
    "\n",
    "This result in **a lot** of tokens to be able to understand our text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common fix is to limit the vocabulary size. Typically, the $n$ most frequent words will be allowed in the vocab. \n",
    "\n",
    "Will render a lot of words **Out of Vocabulary (OOV)**!\n",
    "\n",
    "OOV is typically tokenized with the `<UNK>` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Usage of word tokenizer\n",
    "\n",
    "**Word-based tokenizers** were really popular before the NLP revolution in 2017. \n",
    "\n",
    "Both **GloVe** and **Word2Vec** used word-based tokenization. Focus was much more on the embedding algorithm, rather than the tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As shown, word-based tokenizers have some downsides. \n",
    "\n",
    "## Let's see if we can fix them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Character Based Tokenizers\n",
    "These tokenizers are conceptually very simple. Each character is a token.\n",
    "\n",
    "- `Small vocabulary`. For ASCII, 256. With unicode, this is max 1.1M characters.\n",
    "- No *out of vocabulary* tokens.\n",
    "- Lose a lot of meaninful information\n",
    "- Models typically have `max input lengths` (e.g., 256 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'a', 't']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('Cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mapping to a vocabulary\n",
    "The vocabulary of a text will be really simple, with one index mapping to a letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 0,\n",
       " 'k': 1,\n",
       " 'g': 2,\n",
       " 'i': 3,\n",
       " 'S': 4,\n",
       " 'h': 5,\n",
       " ' ': 6,\n",
       " \"'\": 7,\n",
       " 'u': 8,\n",
       " 'r': 9,\n",
       " 't': 10,\n",
       " 's': 11,\n",
       " 'p': 12,\n",
       " 'a': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'c': 16,\n",
       " '.': 17,\n",
       " 'l': 18,\n",
       " 'e': 19}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{y:x for x, y in enumerate(set(python_zen))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Character based tokenization of Chinese characters\n",
    "*Disclaimer: I do not speak a word of chinese*\n",
    "\n",
    "- Characters have meaning by themselves\n",
    "- But can change by combining characters\n",
    "\n",
    "For example:\n",
    "\n",
    "[È¶¨‰æÜË•ø‰∫û means Malaysia, but taken separately they mean \"Horse come to Western Asia\".](https://medium.com/@jjsham/nlp-tokenizing-chinese-phases-3302da4336bf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some good reads that use character based tokenization:\n",
    "1. [Neural Machine Translation in Linear Time. Kalchbrenner et al. 2017](https://arxiv.org/pdf/1610.10099.pdf)\n",
    "2. [Fully Character-Level Neural Machine Translation without Explicit Segmentation - Lee et al. 2017.](https://aclanthology.org/Q17-1026.pdf)\n",
    "3. [Learning to Generate Reviews and Discovering Sentiment - Radford et al. 2017.](https://arxiv.org/pdf/1704.01444.pdf)\n",
    "\n",
    "From these papers, you can already see that these tokenizers were mainly used a while back, around 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sub-word tokenizers\n",
    "\n",
    "Subword tokenizers combine the two approaches of word and character tokenizers. \n",
    "Keep the advantages of both, limit the disadvantages.\n",
    "\n",
    "Types:\n",
    "- Byte-Pair encoding\n",
    "- WordPiece\n",
    "- Unigram\n",
    "\n",
    "Special case:\n",
    "- SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Byte Pair Encoding (BPE)\n",
    "\n",
    "- Based on a lossless data compression algorithm, proposed by Philip Gage in 1994.\n",
    "- Replace the most common pair of bytes with a byte that does not occur in the data.\n",
    "- First adapted for Neural Machine Translation by [*Sennrich et al.*](https://aclanthology.org/P16-1162.pdf) in 2015.\n",
    "- Used in many SOTA models: Transformer, GPT, GPT-2, XLM, FlauBERT, Roberta.\n",
    "\n",
    "\n",
    "Note: BPE requires the data be split into words already. This can be whitespace splitting, or more advanced like Spacy/NLTK. This step is called pre-tokenization. We'll see why this is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Example from wikipedia\n",
    "We'll encode:\n",
    "\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "The most common pair is `aa`, which will be replace by a byte in the data that is not present, say `Z`. \n",
    "\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We repeat this step, replacing `ab` with `Y`:\n",
    "\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The only byte-pair left is now `ac`, which only occurs once. However, you could continue recursively, replacing `ZY` with `X` if that should occur frequently.\n",
    "\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Adapting this to the NLP domain, the list of replacements (`Z`, `Y` and `X`) can now be considered your vocabulary with tokens, with the slight change that all tokens are initially in the vocabulary, and they are incrementally merged together. \n",
    "\n",
    "**Let's see if we have some time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A natural language example\n",
    "*Note: example taken from the excellent [Huggingface tokenizer summary](https://huggingface.co/docs/transformers/tokenizer_summary)* ü§ó.\n",
    "\n",
    "Let's say that after pre-tokenization, we have the following set of words. With BPE, we need the words and their counts, because we want to merge based on frequency. \n",
    "```python\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "The base vocab consequently is: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Splitting all words into vocabulary symbols, we get:\n",
    "\n",
    "```python\n",
    "(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n",
    "```\n",
    "BPE counts the frequency of all symbol pairs, and picks the most frequent. Here, we have `u` + `g` present $10+5+5=20$ times. \n",
    "\n",
    "Consequently, `ug` is added to the vocabulary, and we replace the `u`+`g` symbols in our words with `ug`. This results in:\n",
    "```python\n",
    "(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n",
    "```\n",
    "\n",
    "And our vocab now is: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameter: `vocab_size`\n",
    "We continue iteratively applying this algorithms until we hit our desired vocabulary size, which is our vocab size + the number of merges. For example, GPT has a vocabulary of 40,478 build up from 478 base characters and 40.000 merges.\n",
    "\n",
    "This learned vocabulary can now be used for unseen text. However, consider that if unknown characters are encountered they will be encoded as `<UNK>` (e.g., `mug` would become `[\"<UNK>\", \"ug\"]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataset specific vocabulary\n",
    "A tokenizer trained on the common crawl will differ from one trained on Wikipedia. For example, GPT is trained on the BooksCorpus (800M words).\n",
    "\n",
    "**Let's have a look at the trained GPT tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 1,\n",
       " ',': 2,\n",
       " 't': 3,\n",
       " 'h': 4,\n",
       " 'e': 5,\n",
       " '\"': 6,\n",
       " 'o': 7,\n",
       " 'a': 8,\n",
       " 'n': 9,\n",
       " 'd': 10,\n",
       " 'i': 11,\n",
       " 'f': 12,\n",
       " 'w': 13,\n",
       " 's': 14,\n",
       " 'y': 15,\n",
       " 'u': 16,\n",
       " 'r': 17,\n",
       " \"'\": 18,\n",
       " '?': 19,\n",
       " 'm': 20,\n",
       " 'b': 21,\n",
       " '-': 22,\n",
       " 'v': 23,\n",
       " 'p': 24,\n",
       " 'c': 25,\n",
       " 'l': 26,\n",
       " 'k': 27,\n",
       " 'j': 28,\n",
       " '!': 29,\n",
       " 'g': 30,\n",
       " '*': 31,\n",
       " ';': 32,\n",
       " ':': 33,\n",
       " 'x': 34,\n",
       " 'q': 35,\n",
       " 'z': 36,\n",
       " ')': 37,\n",
       " '(': 38,\n",
       " '1': 39,\n",
       " '/': 40,\n",
       " '_': 41,\n",
       " '2': 42,\n",
       " '3': 43,\n",
       " '4': 44,\n",
       " '~': 45,\n",
       " '5': 46,\n",
       " '#': 47,\n",
       " '0': 48,\n",
       " '6': 49,\n",
       " '7': 50,\n",
       " '$': 51,\n",
       " '>': 52,\n",
       " '9': 53,\n",
       " '8': 54,\n",
       " '[': 55,\n",
       " ']': 56,\n",
       " '<': 57,\n",
       " '&': 58,\n",
       " '%': 59,\n",
       " '¬®': 60,\n",
       " '`': 61,\n",
       " '√©': 62,\n",
       " '¬ª': 63,\n",
       " '¬´': 64,\n",
       " '=': 65,\n",
       " '‚Ä¢': 66,\n",
       " '@': 67,\n",
       " '+': 68,\n",
       " '¬©': 69,\n",
       " '¬°': 70,\n",
       " '{': 71,\n",
       " '}': 72,\n",
       " '¬™': 73,\n",
       " '√±': 74,\n",
       " '√Ø': 75,\n",
       " '‚Äñ': 76,\n",
       " '√ß': 77,\n",
       " '√≠': 78,\n",
       " '^': 79,\n",
       " '¬£': 80,\n",
       " '¬ß': 81,\n",
       " '‚ô•': 82,\n",
       " '‚àí': 83,\n",
       " '√†': 84,\n",
       " '|': 85,\n",
       " '¬∞': 86,\n",
       " '¬¶': 87,\n",
       " '≈Ç': 88,\n",
       " 'ƒ©': 89,\n",
       " '√º': 90,\n",
       " '¬Æ': 91,\n",
       " '√π': 92,\n",
       " '√°': 93,\n",
       " '√¢': 94,\n",
       " '√≥': 95,\n",
       " '√®': 96,\n",
       " '‚àû': 97,\n",
       " '√´': 98,\n",
       " '√§': 99,\n",
       " '‚ô™': 100,\n",
       " '√≤': 101,\n",
       " 'œâ': 102,\n",
       " '‚ñ™': 103,\n",
       " '¬Ω': 104,\n",
       " '«í': 105,\n",
       " '‚Ä°': 106,\n",
       " '√™': 107,\n",
       " '‚óä': 108,\n",
       " '‚ñ∫': 109,\n",
       " '€û': 110,\n",
       " '√∫': 111,\n",
       " '‚Ç¨': 112,\n",
       " '√¶': 113,\n",
       " '√Æ': 114,\n",
       " '‚Üï': 115,\n",
       " '√¥': 116,\n",
       " 'ƒì': 117,\n",
       " '«ê': 118,\n",
       " '‚ô´': 119,\n",
       " 'ÔøΩ': 120,\n",
       " '\\uf04a': 121,\n",
       " '‚Ñ¢': 122,\n",
       " '≈ô': 123,\n",
       " 'ƒÅ': 124,\n",
       " '¬∑': 125,\n",
       " '¬ø': 126,\n",
       " '\\\\': 127,\n",
       " '‚îÄ': 128,\n",
       " '\\uf067': 129,\n",
       " '\\uf020': 130,\n",
       " '‚àô': 131,\n",
       " '‡•ê': 132,\n",
       " '√∂': 133,\n",
       " '√∏': 134,\n",
       " '\\uf06c': 135,\n",
       " '\\uf09b': 136,\n",
       " '‚óè': 137,\n",
       " '\\xad': 138,\n",
       " '‚ñ†': 139,\n",
       " '\\uf063': 140,\n",
       " '‚Ä†': 141,\n",
       " '√•': 142,\n",
       " '≈ç': 143,\n",
       " '√£': 144,\n",
       " '¬§': 145,\n",
       " '‚†î': 146,\n",
       " '\\uf059': 147,\n",
       " '–Ω': 148,\n",
       " '\\uf09a': 149,\n",
       " '‚öî': 150,\n",
       " 'ƒÉ': 151,\n",
       " '√ª': 152,\n",
       " '¬∫': 153,\n",
       " '‚ô¶': 154,\n",
       " 'ƒù': 155,\n",
       " '¬π': 156,\n",
       " '‚ïê': 157,\n",
       " '\\uf0a3': 158,\n",
       " '¬æ': 159,\n",
       " '√¨': 160,\n",
       " '‚òº': 161,\n",
       " '»ô': 162,\n",
       " '¬º': 163,\n",
       " '‚ò∫': 164,\n",
       " 'ƒë': 165,\n",
       " 'ƒÖ': 166,\n",
       " '«Ω': 167,\n",
       " '‚ï¶': 168,\n",
       " '\\uf02a': 169,\n",
       " '¬¨': 170,\n",
       " 'ƒ´': 171,\n",
       " '\\u200b': 172,\n",
       " '≈ì': 173,\n",
       " '¬¢': 174,\n",
       " '«é': 175,\n",
       " '≈°': 176,\n",
       " ' ª': 177,\n",
       " 'ŒΩ': 178,\n",
       " 'Œ±': 179,\n",
       " '\\uf05d': 180,\n",
       " '—è': 181,\n",
       " '–±': 182,\n",
       " '–π': 183,\n",
       " 'œÑ': 184,\n",
       " 'Œø': 185,\n",
       " 'Œµ': 186,\n",
       " 'ŒØ': 187,\n",
       " 'Œπ': 188,\n",
       " 'Œ¥': 189,\n",
       " '\\uf073': 190,\n",
       " '\\uf05e': 191,\n",
       " '‚Äê': 192,\n",
       " '—Å': 193,\n",
       " '\\uf0a5': 194,\n",
       " '√æ': 195,\n",
       " 'Œ∫': 196,\n",
       " '‚Äë': 197,\n",
       " '‚Äí': 198,\n",
       " '–∫': 199,\n",
       " '\\uf07e': 200,\n",
       " 'œÉ': 201,\n",
       " 'ƒç': 202,\n",
       " '\\uf0be': 203,\n",
       " 'œÖ': 204,\n",
       " 'œå': 205,\n",
       " '\\uf061': 206,\n",
       " '\\uf02d': 207,\n",
       " '√ü': 208,\n",
       " '–µ': 209,\n",
       " '—Ç': 210,\n",
       " 'Œº': 211,\n",
       " 'œÄ': 212,\n",
       " 'œÅ': 213,\n",
       " 'Œ≠': 214,\n",
       " '–≤': 215,\n",
       " '≈ü': 216,\n",
       " '–æ': 217,\n",
       " '√∞': 218,\n",
       " '–∞': 219,\n",
       " 'œÇ': 220,\n",
       " '—Ä': 221,\n",
       " '–º': 222,\n",
       " '—É': 223,\n",
       " 'ŒÆ': 224,\n",
       " 'Œ¨': 225,\n",
       " '–∏': 226,\n",
       " '–¥': 227,\n",
       " '\\uf0bc': 228,\n",
       " '\\uf070': 229,\n",
       " 'Œª': 230,\n",
       " '–ª': 231,\n",
       " 'Œ≥': 232,\n",
       " '¬Ø': 233,\n",
       " '\\uf065': 234,\n",
       " '\\uf043': 235,\n",
       " '\\uf074': 236,\n",
       " '\\uf068': 237,\n",
       " '\\uf072': 238,\n",
       " '.</w>': 239,\n",
       " ',</w>': 240,\n",
       " 't</w>': 241,\n",
       " 'h</w>': 242,\n",
       " 'e</w>': 243,\n",
       " '\"</w>': 244,\n",
       " 'o</w>': 245,\n",
       " 'a</w>': 246,\n",
       " 'n</w>': 247,\n",
       " 'd</w>': 248,\n",
       " 'i</w>': 249,\n",
       " 'f</w>': 250,\n",
       " 'w</w>': 251,\n",
       " 's</w>': 252,\n",
       " 'y</w>': 253,\n",
       " 'u</w>': 254,\n",
       " 'r</w>': 255,\n",
       " \"'</w>\": 256,\n",
       " '?</w>': 257,\n",
       " 'm</w>': 258,\n",
       " 'b</w>': 259,\n",
       " '-</w>': 260,\n",
       " 'v</w>': 261,\n",
       " 'p</w>': 262,\n",
       " 'c</w>': 263,\n",
       " 'l</w>': 264,\n",
       " 'k</w>': 265,\n",
       " 'j</w>': 266,\n",
       " '!</w>': 267,\n",
       " 'g</w>': 268,\n",
       " '*</w>': 269,\n",
       " ';</w>': 270,\n",
       " ':</w>': 271,\n",
       " 'x</w>': 272,\n",
       " 'q</w>': 273,\n",
       " 'z</w>': 274,\n",
       " ')</w>': 275,\n",
       " '(</w>': 276,\n",
       " '1</w>': 277,\n",
       " '/</w>': 278,\n",
       " '_</w>': 279,\n",
       " '2</w>': 280,\n",
       " '3</w>': 281,\n",
       " '4</w>': 282,\n",
       " '~</w>': 283,\n",
       " '5</w>': 284,\n",
       " '#</w>': 285,\n",
       " '0</w>': 286,\n",
       " '6</w>': 287,\n",
       " '7</w>': 288,\n",
       " '$</w>': 289,\n",
       " '></w>': 290,\n",
       " '9</w>': 291,\n",
       " '8</w>': 292,\n",
       " '[</w>': 293,\n",
       " ']</w>': 294,\n",
       " '<</w>': 295,\n",
       " '&</w>': 296,\n",
       " '%</w>': 297,\n",
       " '¬®</w>': 298,\n",
       " '`</w>': 299,\n",
       " '√©</w>': 300,\n",
       " '¬ª</w>': 301,\n",
       " '¬´</w>': 302,\n",
       " '=</w>': 303,\n",
       " '‚Ä¢</w>': 304,\n",
       " '@</w>': 305,\n",
       " '+</w>': 306,\n",
       " '¬©</w>': 307,\n",
       " '¬°</w>': 308,\n",
       " '{</w>': 309,\n",
       " '}</w>': 310,\n",
       " '¬™</w>': 311,\n",
       " '√±</w>': 312,\n",
       " '√Ø</w>': 313,\n",
       " '‚Äñ</w>': 314,\n",
       " '√ß</w>': 315,\n",
       " '√≠</w>': 316,\n",
       " '^</w>': 317,\n",
       " '¬£</w>': 318,\n",
       " '¬ß</w>': 319,\n",
       " '‚ô•</w>': 320,\n",
       " '‚àí</w>': 321,\n",
       " '√†</w>': 322,\n",
       " '|</w>': 323,\n",
       " '¬∞</w>': 324,\n",
       " '¬¶</w>': 325,\n",
       " '≈Ç</w>': 326,\n",
       " 'ƒ©</w>': 327,\n",
       " '√º</w>': 328,\n",
       " '¬Æ</w>': 329,\n",
       " '√π</w>': 330,\n",
       " '√°</w>': 331,\n",
       " '√¢</w>': 332,\n",
       " '√≥</w>': 333,\n",
       " '√®</w>': 334,\n",
       " '‚àû</w>': 335,\n",
       " '√´</w>': 336,\n",
       " '√§</w>': 337,\n",
       " '‚ô™</w>': 338,\n",
       " '√≤</w>': 339,\n",
       " 'œâ</w>': 340,\n",
       " '‚ñ™</w>': 341,\n",
       " '¬Ω</w>': 342,\n",
       " '«í</w>': 343,\n",
       " '‚Ä°</w>': 344,\n",
       " '√™</w>': 345,\n",
       " '‚óä</w>': 346,\n",
       " '‚ñ∫</w>': 347,\n",
       " '€û</w>': 348,\n",
       " '√∫</w>': 349,\n",
       " '‚Ç¨</w>': 350,\n",
       " '√¶</w>': 351,\n",
       " '√Æ</w>': 352,\n",
       " '‚Üï</w>': 353,\n",
       " '√¥</w>': 354,\n",
       " 'ƒì</w>': 355,\n",
       " '«ê</w>': 356,\n",
       " '‚ô´</w>': 357,\n",
       " 'ÔøΩ</w>': 358,\n",
       " '\\uf04a</w>': 359,\n",
       " '‚Ñ¢</w>': 360,\n",
       " '≈ô</w>': 361,\n",
       " 'ƒÅ</w>': 362,\n",
       " '¬∑</w>': 363,\n",
       " '¬ø</w>': 364,\n",
       " '\\\\</w>': 365,\n",
       " '‚îÄ</w>': 366,\n",
       " '\\uf067</w>': 367,\n",
       " '\\uf020</w>': 368,\n",
       " '‚àô</w>': 369,\n",
       " '‡•ê</w>': 370,\n",
       " '√∂</w>': 371,\n",
       " '√∏</w>': 372,\n",
       " '\\uf06c</w>': 373,\n",
       " '\\uf09b</w>': 374,\n",
       " '‚óè</w>': 375,\n",
       " '\\xad</w>': 376,\n",
       " '‚ñ†</w>': 377,\n",
       " '\\uf063</w>': 378,\n",
       " '‚Ä†</w>': 379,\n",
       " '√•</w>': 380,\n",
       " '≈ç</w>': 381,\n",
       " '√£</w>': 382,\n",
       " '¬§</w>': 383,\n",
       " '‚†î</w>': 384,\n",
       " '\\uf059</w>': 385,\n",
       " '–Ω</w>': 386,\n",
       " '\\uf09a</w>': 387,\n",
       " '‚öî</w>': 388,\n",
       " 'ƒÉ</w>': 389,\n",
       " '√ª</w>': 390,\n",
       " '¬∫</w>': 391,\n",
       " '‚ô¶</w>': 392,\n",
       " 'ƒù</w>': 393,\n",
       " '¬π</w>': 394,\n",
       " '‚ïê</w>': 395,\n",
       " '\\uf0a3</w>': 396,\n",
       " '¬æ</w>': 397,\n",
       " '√¨</w>': 398,\n",
       " '‚òº</w>': 399,\n",
       " '»ô</w>': 400,\n",
       " '¬º</w>': 401,\n",
       " '‚ò∫</w>': 402,\n",
       " 'ƒë</w>': 403,\n",
       " 'ƒÖ</w>': 404,\n",
       " '«Ω</w>': 405,\n",
       " '‚ï¶</w>': 406,\n",
       " '\\uf02a</w>': 407,\n",
       " '¬¨</w>': 408,\n",
       " 'ƒ´</w>': 409,\n",
       " '\\u200b</w>': 410,\n",
       " '≈ì</w>': 411,\n",
       " '¬¢</w>': 412,\n",
       " '«é</w>': 413,\n",
       " '≈°</w>': 414,\n",
       " ' ª</w>': 415,\n",
       " 'ŒΩ</w>': 416,\n",
       " 'Œ±</w>': 417,\n",
       " '\\uf05d</w>': 418,\n",
       " '—è</w>': 419,\n",
       " '–±</w>': 420,\n",
       " '–π</w>': 421,\n",
       " 'œÑ</w>': 422,\n",
       " 'Œø</w>': 423,\n",
       " 'Œµ</w>': 424,\n",
       " 'ŒØ</w>': 425,\n",
       " 'Œπ</w>': 426,\n",
       " 'Œ¥</w>': 427,\n",
       " '\\uf073</w>': 428,\n",
       " '\\uf05e</w>': 429,\n",
       " '‚Äê</w>': 430,\n",
       " '—Å</w>': 431,\n",
       " '\\uf0a5</w>': 432,\n",
       " '√æ</w>': 433,\n",
       " 'Œ∫</w>': 434,\n",
       " '‚Äë</w>': 435,\n",
       " '‚Äí</w>': 436,\n",
       " '–∫</w>': 437,\n",
       " '\\uf07e</w>': 438,\n",
       " 'œÉ</w>': 439,\n",
       " 'ƒç</w>': 440,\n",
       " '\\uf0be</w>': 441,\n",
       " 'œÖ</w>': 442,\n",
       " 'œå</w>': 443,\n",
       " '\\uf061</w>': 444,\n",
       " '\\uf02d</w>': 445,\n",
       " '√ü</w>': 446,\n",
       " '–µ</w>': 447,\n",
       " '—Ç</w>': 448,\n",
       " 'Œº</w>': 449,\n",
       " 'œÄ</w>': 450,\n",
       " 'œÅ</w>': 451,\n",
       " 'Œ≠</w>': 452,\n",
       " '–≤</w>': 453,\n",
       " '≈ü</w>': 454,\n",
       " '–æ</w>': 455,\n",
       " '√∞</w>': 456,\n",
       " '–∞</w>': 457,\n",
       " 'œÇ</w>': 458,\n",
       " '—Ä</w>': 459,\n",
       " '–º</w>': 460,\n",
       " '—É</w>': 461,\n",
       " 'ŒÆ</w>': 462,\n",
       " 'Œ¨</w>': 463,\n",
       " '–∏</w>': 464,\n",
       " '–¥</w>': 465,\n",
       " '\\uf0bc</w>': 466,\n",
       " '\\uf070</w>': 467,\n",
       " 'Œª</w>': 468,\n",
       " '–ª</w>': 469,\n",
       " 'Œ≥</w>': 470,\n",
       " '¬Ø</w>': 471,\n",
       " '\\uf065</w>': 472,\n",
       " '\\uf043</w>': 473,\n",
       " '\\uf074</w>': 474,\n",
       " '\\uf068</w>': 475,\n",
       " '\\uf072</w>': 476,\n",
       " 'th': 477,\n",
       " 'in': 478,\n",
       " 'ed</w>': 479,\n",
       " 'an': 480,\n",
       " 'the</w>': 481,\n",
       " 'ou': 482,\n",
       " 'er</w>': 483,\n",
       " 'ing</w>': 484,\n",
       " 'to</w>': 485,\n",
       " 'er': 486,\n",
       " 'he</w>': 487,\n",
       " 'and</w>': 488,\n",
       " 'ar': 489,\n",
       " 'hi': 490,\n",
       " 'at</w>': 491,\n",
       " 're': 492,\n",
       " 'wa': 493,\n",
       " 'on': 494,\n",
       " 'st': 495,\n",
       " 'en': 496,\n",
       " 'ha': 497,\n",
       " 'of</w>': 498,\n",
       " 'or': 499,\n",
       " 'in</w>': 500,\n",
       " 'al': 501,\n",
       " 'it': 502,\n",
       " 'en</w>': 503,\n",
       " 'on</w>': 504,\n",
       " 'el': 505,\n",
       " 'ro': 506,\n",
       " 'it</w>': 507,\n",
       " 'ac': 508,\n",
       " 'was</w>': 509,\n",
       " 'me</w>': 510,\n",
       " 'yo': 511,\n",
       " 'you</w>': 512,\n",
       " 'her</w>': 513,\n",
       " 'es</w>': 514,\n",
       " 'ly</w>': 515,\n",
       " 'no': 516,\n",
       " 'at': 517,\n",
       " 'lo': 518,\n",
       " 'li': 519,\n",
       " 'she</w>': 520,\n",
       " 'wh': 521,\n",
       " 'or</w>': 522,\n",
       " 'st</w>': 523,\n",
       " 'his</w>': 524,\n",
       " 'that</w>': 525,\n",
       " 'ea': 526,\n",
       " 've</w>': 527,\n",
       " 'be': 528,\n",
       " 'ri': 529,\n",
       " 'ld</w>': 530,\n",
       " 'an</w>': 531,\n",
       " 'gh': 532,\n",
       " 'ere</w>': 533,\n",
       " 'the': 534,\n",
       " \"'s</w>\": 535,\n",
       " 'ti': 536,\n",
       " \"'t</w>\": 537,\n",
       " \"n't</w>\": 538,\n",
       " 'id</w>': 539,\n",
       " 'sa': 540,\n",
       " 'le</w>': 541,\n",
       " 'si': 542,\n",
       " 'ur': 543,\n",
       " 'is</w>': 544,\n",
       " 'bu': 545,\n",
       " 'se</w>': 546,\n",
       " 'my</w>': 547,\n",
       " 'ho': 548,\n",
       " 'ould</w>': 549,\n",
       " 'ne': 550,\n",
       " 'out</w>': 551,\n",
       " 'le': 552,\n",
       " 'wit': 553,\n",
       " 'om': 554,\n",
       " 'il': 555,\n",
       " 'with</w>': 556,\n",
       " 'as</w>': 557,\n",
       " 'had</w>': 558,\n",
       " 'se': 559,\n",
       " 'ght</w>': 560,\n",
       " 'ke</w>': 561,\n",
       " 'for</w>': 562,\n",
       " 'un': 563,\n",
       " 'la': 564,\n",
       " 'ra': 565,\n",
       " 'one</w>': 566,\n",
       " 'ma': 567,\n",
       " 'but</w>': 568,\n",
       " 'do': 569,\n",
       " 'ab': 570,\n",
       " 'to': 571,\n",
       " 'ic': 572,\n",
       " 'ch': 573,\n",
       " 'ev': 574,\n",
       " 'him</w>': 575,\n",
       " 'sh': 576,\n",
       " 'ked</w>': 577,\n",
       " 'ca': 578,\n",
       " 'pp': 579,\n",
       " 'be</w>': 580,\n",
       " 'go': 581,\n",
       " 'sp': 582,\n",
       " 'oun': 583,\n",
       " 'ir': 584,\n",
       " 'de': 585,\n",
       " 'ther</w>': 586,\n",
       " 'do</w>': 587,\n",
       " 'co': 588,\n",
       " 'all</w>': 589,\n",
       " 'et</w>': 590,\n",
       " 'ss</w>': 591,\n",
       " 'di': 592,\n",
       " 'mo': 593,\n",
       " 'ent</w>': 594,\n",
       " 'not</w>': 595,\n",
       " 'de</w>': 596,\n",
       " 'now</w>': 597,\n",
       " 'ted</w>': 598,\n",
       " 'what</w>': 599,\n",
       " 'they</w>': 600,\n",
       " 'ag': 601,\n",
       " 'ack</w>': 602,\n",
       " 'said</w>': 603,\n",
       " 'have</w>': 604,\n",
       " 'fro': 605,\n",
       " 'we</w>': 606,\n",
       " 'ch</w>': 607,\n",
       " 'ce</w>': 608,\n",
       " 'up</w>': 609,\n",
       " 'ore</w>': 610,\n",
       " 'bo': 611,\n",
       " 'ver</w>': 612,\n",
       " 'ter</w>': 613,\n",
       " 'loo': 614,\n",
       " 'thing</w>': 615,\n",
       " 'this</w>': 616,\n",
       " 'from</w>': 617,\n",
       " 'king</w>': 618,\n",
       " 'ds</w>': 619,\n",
       " 'so</w>': 620,\n",
       " 'as': 621,\n",
       " 'our</w>': 622,\n",
       " 'su': 623,\n",
       " 'wn</w>': 624,\n",
       " 'con': 625,\n",
       " 'did</w>': 626,\n",
       " 'mi': 627,\n",
       " 'ru': 628,\n",
       " 'fe': 629,\n",
       " 'sed</w>': 630,\n",
       " 'gh</w>': 631,\n",
       " 'ta': 632,\n",
       " 'ju': 633,\n",
       " 'led</w>': 634,\n",
       " 'could</w>': 635,\n",
       " 'would</w>': 636,\n",
       " 'so': 637,\n",
       " 'way</w>': 638,\n",
       " 'ts</w>': 639,\n",
       " 'are</w>': 640,\n",
       " 'were</w>': 641,\n",
       " 'ir</w>': 642,\n",
       " 'da': 643,\n",
       " 'po': 644,\n",
       " 'if</w>': 645,\n",
       " 'em': 646,\n",
       " 'ill</w>': 647,\n",
       " 'rea': 648,\n",
       " 'like</w>': 649,\n",
       " 'ers</w>': 650,\n",
       " 'back</w>': 651,\n",
       " 'wor': 652,\n",
       " 'ear': 653,\n",
       " 'ound</w>': 654,\n",
       " 'there</w>': 655,\n",
       " \"'d</w>\": 656,\n",
       " 'ded</w>': 657,\n",
       " 'ell</w>': 658,\n",
       " 'ex': 659,\n",
       " 'qu': 660,\n",
       " 'ough</w>': 661,\n",
       " 'hea': 662,\n",
       " 'th</w>': 663,\n",
       " 'no</w>': 664,\n",
       " 'll</w>': 665,\n",
       " 'into</w>': 666,\n",
       " 'ing': 667,\n",
       " 'just</w>': 668,\n",
       " 'when</w>': 669,\n",
       " 'about</w>': 670,\n",
       " 'ati': 671,\n",
       " 'fa': 672,\n",
       " 'pu': 673,\n",
       " 'then</w>': 674,\n",
       " 'ally</w>': 675,\n",
       " 'sc': 676,\n",
       " 'lea': 677,\n",
       " 'ver': 678,\n",
       " 'al</w>': 679,\n",
       " 'mu': 680,\n",
       " 'ant</w>': 681,\n",
       " 'ace</w>': 682,\n",
       " 'fu': 683,\n",
       " 'whi': 684,\n",
       " 'yes</w>': 685,\n",
       " 'ind</w>': 686,\n",
       " 'ting</w>': 687,\n",
       " 'them</w>': 688,\n",
       " 'dy</w>': 689,\n",
       " 'com': 690,\n",
       " 'ding</w>': 691,\n",
       " 'gu': 692,\n",
       " 'tur': 693,\n",
       " 'been</w>': 694,\n",
       " 'ee': 695,\n",
       " 'for': 696,\n",
       " 'som': 697,\n",
       " 'ard</w>': 698,\n",
       " 'know</w>': 699,\n",
       " 'some': 700,\n",
       " 'op': 701,\n",
       " 'by</w>': 702,\n",
       " 'tw': 703,\n",
       " 'your</w>': 704,\n",
       " 'ter': 705,\n",
       " 'pro': 706,\n",
       " 'sel': 707,\n",
       " 'of': 708,\n",
       " 'ge</w>': 709,\n",
       " 'fi': 710,\n",
       " 'od</w>': 711,\n",
       " 'pa': 712,\n",
       " 'ec': 713,\n",
       " 'down</w>': 714,\n",
       " 'over</w>': 715,\n",
       " 're</w>': 716,\n",
       " 'lu': 717,\n",
       " 'how</w>': 718,\n",
       " \"'m</w>\": 719,\n",
       " 'time</w>': 720,\n",
       " 'aga': 721,\n",
       " 'wi': 722,\n",
       " 'tr': 723,\n",
       " 'sur': 724,\n",
       " 'more</w>': 725,\n",
       " '..': 726,\n",
       " 'get</w>': 727,\n",
       " 'other</w>': 728,\n",
       " 'pre': 729,\n",
       " 'ned</w>': 730,\n",
       " 'ong</w>': 731,\n",
       " 'der</w>': 732,\n",
       " 'vi': 733,\n",
       " 'par': 734,\n",
       " 'ys</w>': 735,\n",
       " 'pl': 736,\n",
       " 'side</w>': 737,\n",
       " 'fo': 738,\n",
       " 'tly</w>': 739,\n",
       " 'ck</w>': 740,\n",
       " 'eyes</w>': 741,\n",
       " 'ks</w>': 742,\n",
       " 'gi': 743,\n",
       " 'me': 744,\n",
       " 'ine</w>': 745,\n",
       " 'ate</w>': 746,\n",
       " 'ni': 747,\n",
       " 'self</w>': 748,\n",
       " '...</w>': 749,\n",
       " 'per': 750,\n",
       " 'ty</w>': 751,\n",
       " 'af': 752,\n",
       " 'el</w>': 753,\n",
       " 'their</w>': 754,\n",
       " 'ice</w>': 755,\n",
       " 'head</w>': 756,\n",
       " 'thin': 757,\n",
       " 'pped</w>': 758,\n",
       " 'can</w>': 759,\n",
       " 'gr': 760,\n",
       " \"'re</w>\": 761,\n",
       " 'man</w>': 762,\n",
       " 'who</w>': 763,\n",
       " 'ying</w>': 764,\n",
       " 'ling</w>': 765,\n",
       " 'ation</w>': 766,\n",
       " 'sto': 767,\n",
       " 'us</w>': 768,\n",
       " 'sm': 769,\n",
       " 'right</w>': 770,\n",
       " 'der': 771,\n",
       " 'sho': 772,\n",
       " 'ok</w>': 773,\n",
       " 'ge': 774,\n",
       " 'any</w>': 775,\n",
       " 'ga': 776,\n",
       " 'fore</w>': 777,\n",
       " 'pe': 778,\n",
       " 'ever': 779,\n",
       " 'ought</w>': 780,\n",
       " 'before</w>': 781,\n",
       " 'han': 782,\n",
       " 'new</w>': 783,\n",
       " 'even</w>': 784,\n",
       " 'around</w>': 785,\n",
       " 'ely</w>': 786,\n",
       " 'mp': 787,\n",
       " 'see</w>': 788,\n",
       " 'star': 789,\n",
       " 'cau': 790,\n",
       " 'any': 791,\n",
       " 'ved</w>': 792,\n",
       " 'here</w>': 793,\n",
       " 'ss': 794,\n",
       " 'sh</w>': 795,\n",
       " 'clo': 796,\n",
       " 'going</w>': 797,\n",
       " 'fir': 798,\n",
       " 'go</w>': 799,\n",
       " 'our': 800,\n",
       " 'thr': 801,\n",
       " 'ps</w>': 802,\n",
       " 'some</w>': 803,\n",
       " \"'ll</w>\": 804,\n",
       " 'low': 805,\n",
       " 'where</w>': 806,\n",
       " 'ving</w>': 807,\n",
       " 'only</w>': 808,\n",
       " 'tion</w>': 809,\n",
       " 'hel': 810,\n",
       " 'off</w>': 811,\n",
       " 'will</w>': 812,\n",
       " 'na': 813,\n",
       " 'ci': 814,\n",
       " 'than</w>': 815,\n",
       " 'looked</w>': 816,\n",
       " 'able</w>': 817,\n",
       " 'tle</w>': 818,\n",
       " 'roo': 819,\n",
       " 'ons</w>': 820,\n",
       " 'ten': 821,\n",
       " 'through</w>': 822,\n",
       " 'want</w>': 823,\n",
       " 'ous</w>': 824,\n",
       " 'think</w>': 825,\n",
       " 'ning</w>': 826,\n",
       " 'cu': 827,\n",
       " 'hand</w>': 828,\n",
       " 'ba': 829,\n",
       " 'vo': 830,\n",
       " 'mar': 831,\n",
       " 'jo': 832,\n",
       " 'again</w>': 833,\n",
       " 'too</w>': 834,\n",
       " 'face</w>': 835,\n",
       " 'te</w>': 836,\n",
       " 'wal': 837,\n",
       " 'shi': 838,\n",
       " 'sw': 839,\n",
       " 'lit': 840,\n",
       " 'away</w>': 841,\n",
       " 'ft</w>': 842,\n",
       " 'still</w>': 843,\n",
       " 'room</w>': 844,\n",
       " 'ity</w>': 845,\n",
       " 'something</w>': 846,\n",
       " 'fe</w>': 847,\n",
       " 'come</w>': 848,\n",
       " 'ssi': 849,\n",
       " 'day</w>': 850,\n",
       " 'let</w>': 851,\n",
       " 'ry</w>': 852,\n",
       " 'ear</w>': 853,\n",
       " 'ep': 854,\n",
       " 'ings</w>': 855,\n",
       " 'gre': 856,\n",
       " 'car': 857,\n",
       " 'ered</w>': 858,\n",
       " 'est': 859,\n",
       " 'wan': 860,\n",
       " 'after</w>': 861,\n",
       " 'well</w>': 862,\n",
       " 'hear': 863,\n",
       " 'asked</w>': 864,\n",
       " 'bl': 865,\n",
       " 'thought</w>': 866,\n",
       " 'two</w>': 867,\n",
       " 'never</w>': 868,\n",
       " 'ang': 869,\n",
       " 'good</w>': 870,\n",
       " 'ever</w>': 871,\n",
       " 'end</w>': 872,\n",
       " 'sta': 873,\n",
       " 'ad': 874,\n",
       " 'ated</w>': 875,\n",
       " 'br': 876,\n",
       " 'ance</w>': 877,\n",
       " 'min': 878,\n",
       " 'cha': 879,\n",
       " \"'ve</w>\": 880,\n",
       " 'sure</w>': 881,\n",
       " 'ck': 882,\n",
       " 'cause</w>': 883,\n",
       " 'hu': 884,\n",
       " 'made</w>': 885,\n",
       " 'got</w>': 886,\n",
       " 'tri': 887,\n",
       " 'ssed</w>': 888,\n",
       " 'much</w>': 889,\n",
       " 'look</w>': 890,\n",
       " 'ched</w>': 891,\n",
       " 'mb': 892,\n",
       " 'shed</w>': 893,\n",
       " 'fin': 894,\n",
       " 'why</w>': 895,\n",
       " 'du': 896,\n",
       " 'ward</w>': 897,\n",
       " 'bel': 898,\n",
       " 'turned</w>': 899,\n",
       " 'sha': 900,\n",
       " 'gg': 901,\n",
       " 'ach</w>': 902,\n",
       " 'bro': 903,\n",
       " 'gra': 904,\n",
       " 'most</w>': 905,\n",
       " 'knew</w>': 906,\n",
       " 'ath</w>': 907,\n",
       " 'door</w>': 908,\n",
       " 'little</w>': 909,\n",
       " 'tal': 910,\n",
       " 'ls</w>': 911,\n",
       " 'because</w>': 912,\n",
       " 'fel': 913,\n",
       " 'ened</w>': 914,\n",
       " 'tu': 915,\n",
       " 'war': 916,\n",
       " 'te': 917,\n",
       " 'sk': 918,\n",
       " 'ff': 919,\n",
       " 'sit': 920,\n",
       " 'take</w>': 921,\n",
       " 'happ': 922,\n",
       " 'man': 923,\n",
       " 'ms</w>': 924,\n",
       " 'make</w>': 925,\n",
       " 'cal': 926,\n",
       " 'every': 927,\n",
       " 'long</w>': 928,\n",
       " 'first</w>': 929,\n",
       " 'tra': 930,\n",
       " 'ach': 931,\n",
       " 'ste': 932,\n",
       " 'ful</w>': 933,\n",
       " 'ble</w>': 934,\n",
       " 'ess</w>': 935,\n",
       " 'im': 936,\n",
       " 'say</w>': 937,\n",
       " 'ence</w>': 938,\n",
       " 'came</w>': 939,\n",
       " 'ced</w>': 940,\n",
       " 'pri': 941,\n",
       " 'felt</w>': 942,\n",
       " 'bed</w>': 943,\n",
       " 'ree</w>': 944,\n",
       " 'son</w>': 945,\n",
       " 'mon': 946,\n",
       " 'dar': 947,\n",
       " 'took</w>': 948,\n",
       " 'ser': 949,\n",
       " 'app': 950,\n",
       " 'ki': 951,\n",
       " 'tru': 952,\n",
       " 'fri': 953,\n",
       " 'low</w>': 954,\n",
       " 'chi': 955,\n",
       " 'body</w>': 956,\n",
       " 'fr': 957,\n",
       " 'pla': 958,\n",
       " 'sin': 959,\n",
       " 'ali': 960,\n",
       " 'wanted</w>': 961,\n",
       " 'ose</w>': 962,\n",
       " 'very</w>': 963,\n",
       " 'ves</w>': 964,\n",
       " 'est</w>': 965,\n",
       " 'need</w>': 966,\n",
       " 'pul': 967,\n",
       " 'kno': 968,\n",
       " 'ears</w>': 969,\n",
       " 'dd': 970,\n",
       " 'stu': 971,\n",
       " 'tell</w>': 972,\n",
       " 'pi': 973,\n",
       " 'str': 974,\n",
       " 'dre': 975,\n",
       " 'really</w>': 976,\n",
       " 'cre': 977,\n",
       " 'red</w>': 978,\n",
       " 'bi': 979,\n",
       " 'has</w>': 980,\n",
       " 'cont': 981,\n",
       " 'he': 982,\n",
       " 'hands</w>': 983,\n",
       " 'which</w>': 984,\n",
       " 'sen': 985,\n",
       " 'peop': 986,\n",
       " 'its</w>': 987,\n",
       " 'sing</w>': 988,\n",
       " 'people</w>': 989,\n",
       " 'rec': 990,\n",
       " 'wat': 991,\n",
       " 'sli': 992,\n",
       " 'ca</w>': 993,\n",
       " 'should</w>': 994,\n",
       " 'night</w>': 995,\n",
       " 'ws</w>': 996,\n",
       " 'with': 997,\n",
       " 'though</w>': 998,\n",
       " 'left</w>': 999,\n",
       " 'while</w>': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "bpe_tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "print(len(bpe_tokenizer.get_vocab()))\n",
    "bpe_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['special</w>',\n",
       " 'cases</w>',\n",
       " 'are</w>',\n",
       " \"n't</w>\",\n",
       " 'special</w>',\n",
       " 'enough</w>',\n",
       " 'to</w>',\n",
       " 'break</w>',\n",
       " 'the</w>',\n",
       " 'rules</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.tokenize(python_zen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m.</w>', 'night</w>', 'shy', 'am', 'alan</w>']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like directors\n",
    "bpe_tokenizer.tokenize('M. Night Shyamalan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':</w>', 'hippo', 'po', 'tom', 'on', 'stro', 'se', 'squi', 'pp', 'ed', 'ali', 'op', 'ho', 'bia</w>']\n"
     ]
    }
   ],
   "source": [
    "# The fear of long words\n",
    "print(bpe_tokenizer.tokenize(\":Hippopotomonstrosesquippedaliophobia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['projec', 't', 'contrac', 't', 'char', 'ging', 'peri', 'od', 'projec', 't', 'accoun', 'tre', 'fer', 'en', 'c', 'ev', 'm</w>']\n"
     ]
    }
   ],
   "source": [
    "# A very long Java Class name\n",
    "print(bpe_tokenizer.tokenize('ProjectContractChargingPeriodProjectAccountReferenceVM'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2983, 5767, 640, 538, 2983, 1046, 485, 2158, 481, 4556, 239], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer(python_zen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Looks a lot like what we had at the start ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordPiece\n",
    "- Used by: BERT, distilBERT, Electra\n",
    "- Similar to BPE: starts with a base vocabulary that includes every character, but merges based on maximum likelihood of the training data, rather than just frequency. \n",
    "- Is a Google internal model without Open Source implementation. SentencePiece is Google's open source version of this. We don't really know how much they differ. \n",
    "- First proposed by [Schuster and Nakajima, 2015](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), in the paper called *Japanese and Korean Voice Search*. Both working at Google. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparisons with other implementations\n",
    "\n",
    "|Feature|SentencePiece|[subword-nmt](https://github.com/rsennrich/subword-nmt)|[WordPiece](https://arxiv.org/pdf/1609.08144.pdf)|\n",
    "|:---|:---:|:---:|:---:|\n",
    "|Supported algorithm|BPE, unigram, char, word|BPE|BPE*|\n",
    "|OSS?|Yes|Yes|Google internal|\n",
    "|Subword regularization|[Yes](#subword-regularization)|No|No|\n",
    "|Python Library (pip)|[Yes](python/README.md)|No|N/A|\n",
    "|C++ Library|[Yes](doc/api.md)|No|N/A|\n",
    "|Pre-segmentation required?|[No](#whitespace-is-treated-as-a-basic-symbol)|Yes|Yes|\n",
    "|Customizable normalization (e.g., NFKC)|[Yes](doc/normalization.md)|No|N/A|\n",
    "|Direct id generation|[Yes](#end-to-end-example)|No|N/A|\n",
    "\n",
    "*From the [SentencePiece Github](https://github.com/google/sentencepiece).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Differences with BPE\n",
    "- Subwords are identified with a prefix (`##`)\n",
    "- If no subword can be found in the vocabulary, the whole word is tokenized as `[UNK]`. So the previous example of encoding `mug` as `[\"<UNK>\", \"ug\"]` cannot happen with wordpiece. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Merging rules\n",
    "Search for the symbol pair with the highest score. This prioritizes combining pairs that aren't as frequent in the vocabulary. \n",
    "$$ score_{ij} = \\frac{f_{ij}}{f_i \\cdot f_j} $$\n",
    "\n",
    "If $f_i \\cdot f_j$ is very small, the score becomes very large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So say we have the same words as before:\n",
    "```yaml\n",
    "Corpus: (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "\n",
    "Tokenizing with the start vocabulary:\n",
    "```yaml\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]\n",
    "Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. The most frequent pair is (`##u`, `##g`), 20 times, but the individual frequency of `##u` is very high. \n",
    "2. The result is that the pair's score is only $\\frac{20}{36\\times20} = \\frac{1}{36}$. \n",
    "3. This goes for all combinations with `##u`. \n",
    "4. The highest scoring pair is (`##g`, `##s`) - at $\\frac{5}{20\\times5}=\\frac{1}{20}$. This is also the only pair without `##u` in it. \n",
    "\n",
    "First merge is (`##g`, `##s`) --> (`##gs`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then get, with our new token:\n",
    "```yaml\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"]\n",
    "Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5)\n",
    "```\n",
    "\n",
    "And the next step, where `hu` is the token with the highest likelihood increase to the corpus. \n",
    "\n",
    "```yaml\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"]\n",
    "Corpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, we apply this iteratively until we have our desired vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[unused1]': 1,\n",
       " '[unused2]': 2,\n",
       " '[unused3]': 3,\n",
       " '[unused4]': 4,\n",
       " '[unused5]': 5,\n",
       " '[unused6]': 6,\n",
       " '[unused7]': 7,\n",
       " '[unused8]': 8,\n",
       " '[unused9]': 9,\n",
       " '[unused10]': 10,\n",
       " '[unused11]': 11,\n",
       " '[unused12]': 12,\n",
       " '[unused13]': 13,\n",
       " '[unused14]': 14,\n",
       " '[unused15]': 15,\n",
       " '[unused16]': 16,\n",
       " '[unused17]': 17,\n",
       " '[unused18]': 18,\n",
       " '[unused19]': 19,\n",
       " '[unused20]': 20,\n",
       " '[unused21]': 21,\n",
       " '[unused22]': 22,\n",
       " '[unused23]': 23,\n",
       " '[unused24]': 24,\n",
       " '[unused25]': 25,\n",
       " '[unused26]': 26,\n",
       " '[unused27]': 27,\n",
       " '[unused28]': 28,\n",
       " '[unused29]': 29,\n",
       " '[unused30]': 30,\n",
       " '[unused31]': 31,\n",
       " '[unused32]': 32,\n",
       " '[unused33]': 33,\n",
       " '[unused34]': 34,\n",
       " '[unused35]': 35,\n",
       " '[unused36]': 36,\n",
       " '[unused37]': 37,\n",
       " '[unused38]': 38,\n",
       " '[unused39]': 39,\n",
       " '[unused40]': 40,\n",
       " '[unused41]': 41,\n",
       " '[unused42]': 42,\n",
       " '[unused43]': 43,\n",
       " '[unused44]': 44,\n",
       " '[unused45]': 45,\n",
       " '[unused46]': 46,\n",
       " '[unused47]': 47,\n",
       " '[unused48]': 48,\n",
       " '[unused49]': 49,\n",
       " '[unused50]': 50,\n",
       " '[unused51]': 51,\n",
       " '[unused52]': 52,\n",
       " '[unused53]': 53,\n",
       " '[unused54]': 54,\n",
       " '[unused55]': 55,\n",
       " '[unused56]': 56,\n",
       " '[unused57]': 57,\n",
       " '[unused58]': 58,\n",
       " '[unused59]': 59,\n",
       " '[unused60]': 60,\n",
       " '[unused61]': 61,\n",
       " '[unused62]': 62,\n",
       " '[unused63]': 63,\n",
       " '[unused64]': 64,\n",
       " '[unused65]': 65,\n",
       " '[unused66]': 66,\n",
       " '[unused67]': 67,\n",
       " '[unused68]': 68,\n",
       " '[unused69]': 69,\n",
       " '[unused70]': 70,\n",
       " '[unused71]': 71,\n",
       " '[unused72]': 72,\n",
       " '[unused73]': 73,\n",
       " '[unused74]': 74,\n",
       " '[unused75]': 75,\n",
       " '[unused76]': 76,\n",
       " '[unused77]': 77,\n",
       " '[unused78]': 78,\n",
       " '[unused79]': 79,\n",
       " '[unused80]': 80,\n",
       " '[unused81]': 81,\n",
       " '[unused82]': 82,\n",
       " '[unused83]': 83,\n",
       " '[unused84]': 84,\n",
       " '[unused85]': 85,\n",
       " '[unused86]': 86,\n",
       " '[unused87]': 87,\n",
       " '[unused88]': 88,\n",
       " '[unused89]': 89,\n",
       " '[unused90]': 90,\n",
       " '[unused91]': 91,\n",
       " '[unused92]': 92,\n",
       " '[unused93]': 93,\n",
       " '[unused94]': 94,\n",
       " '[unused95]': 95,\n",
       " '[unused96]': 96,\n",
       " '[unused97]': 97,\n",
       " '[unused98]': 98,\n",
       " '[unused99]': 99,\n",
       " '[UNK]': 100,\n",
       " '[CLS]': 101,\n",
       " '[SEP]': 102,\n",
       " '[MASK]': 103,\n",
       " '[unused100]': 104,\n",
       " '[unused101]': 105,\n",
       " '!': 106,\n",
       " '\"': 107,\n",
       " '#': 108,\n",
       " '$': 109,\n",
       " '%': 110,\n",
       " '&': 111,\n",
       " \"'\": 112,\n",
       " '(': 113,\n",
       " ')': 114,\n",
       " '*': 115,\n",
       " '+': 116,\n",
       " ',': 117,\n",
       " '-': 118,\n",
       " '.': 119,\n",
       " '/': 120,\n",
       " '0': 121,\n",
       " '1': 122,\n",
       " '2': 123,\n",
       " '3': 124,\n",
       " '4': 125,\n",
       " '5': 126,\n",
       " '6': 127,\n",
       " '7': 128,\n",
       " '8': 129,\n",
       " '9': 130,\n",
       " ':': 131,\n",
       " ';': 132,\n",
       " '<': 133,\n",
       " '=': 134,\n",
       " '>': 135,\n",
       " '?': 136,\n",
       " '@': 137,\n",
       " 'A': 138,\n",
       " 'B': 139,\n",
       " 'C': 140,\n",
       " 'D': 141,\n",
       " 'E': 142,\n",
       " 'F': 143,\n",
       " 'G': 144,\n",
       " 'H': 145,\n",
       " 'I': 146,\n",
       " 'J': 147,\n",
       " 'K': 148,\n",
       " 'L': 149,\n",
       " 'M': 150,\n",
       " 'N': 151,\n",
       " 'O': 152,\n",
       " 'P': 153,\n",
       " 'Q': 154,\n",
       " 'R': 155,\n",
       " 'S': 156,\n",
       " 'T': 157,\n",
       " 'U': 158,\n",
       " 'V': 159,\n",
       " 'W': 160,\n",
       " 'X': 161,\n",
       " 'Y': 162,\n",
       " 'Z': 163,\n",
       " '[': 164,\n",
       " '\\\\': 165,\n",
       " ']': 166,\n",
       " '^': 167,\n",
       " '_': 168,\n",
       " '`': 169,\n",
       " 'a': 170,\n",
       " 'b': 171,\n",
       " 'c': 172,\n",
       " 'd': 173,\n",
       " 'e': 174,\n",
       " 'f': 175,\n",
       " 'g': 176,\n",
       " 'h': 177,\n",
       " 'i': 178,\n",
       " 'j': 179,\n",
       " 'k': 180,\n",
       " 'l': 181,\n",
       " 'm': 182,\n",
       " 'n': 183,\n",
       " 'o': 184,\n",
       " 'p': 185,\n",
       " 'q': 186,\n",
       " 'r': 187,\n",
       " 's': 188,\n",
       " 't': 189,\n",
       " 'u': 190,\n",
       " 'v': 191,\n",
       " 'w': 192,\n",
       " 'x': 193,\n",
       " 'y': 194,\n",
       " 'z': 195,\n",
       " '{': 196,\n",
       " '|': 197,\n",
       " '}': 198,\n",
       " '~': 199,\n",
       " '¬°': 200,\n",
       " '¬¢': 201,\n",
       " '¬£': 202,\n",
       " '¬•': 203,\n",
       " '¬ß': 204,\n",
       " '¬®': 205,\n",
       " '¬©': 206,\n",
       " '¬™': 207,\n",
       " '¬´': 208,\n",
       " '¬¨': 209,\n",
       " '¬Æ': 210,\n",
       " '¬∞': 211,\n",
       " '¬±': 212,\n",
       " '¬≤': 213,\n",
       " '¬≥': 214,\n",
       " '¬¥': 215,\n",
       " '¬µ': 216,\n",
       " '¬∂': 217,\n",
       " '¬∑': 218,\n",
       " '¬π': 219,\n",
       " '¬∫': 220,\n",
       " '¬ª': 221,\n",
       " '¬º': 222,\n",
       " '¬Ω': 223,\n",
       " '¬æ': 224,\n",
       " '¬ø': 225,\n",
       " '√Ä': 226,\n",
       " '√Å': 227,\n",
       " '√Ç': 228,\n",
       " '√Ñ': 229,\n",
       " '√Ö': 230,\n",
       " '√Ü': 231,\n",
       " '√á': 232,\n",
       " '√à': 233,\n",
       " '√â': 234,\n",
       " '√ç': 235,\n",
       " '√é': 236,\n",
       " '√ë': 237,\n",
       " '√ì': 238,\n",
       " '√ñ': 239,\n",
       " '√ó': 240,\n",
       " '√ò': 241,\n",
       " '√ö': 242,\n",
       " '√ú': 243,\n",
       " '√û': 244,\n",
       " '√ü': 245,\n",
       " '√†': 246,\n",
       " '√°': 247,\n",
       " '√¢': 248,\n",
       " '√£': 249,\n",
       " '√§': 250,\n",
       " '√•': 251,\n",
       " '√¶': 252,\n",
       " '√ß': 253,\n",
       " '√®': 254,\n",
       " '√©': 255,\n",
       " '√™': 256,\n",
       " '√´': 257,\n",
       " '√¨': 258,\n",
       " '√≠': 259,\n",
       " '√Æ': 260,\n",
       " '√Ø': 261,\n",
       " '√∞': 262,\n",
       " '√±': 263,\n",
       " '√≤': 264,\n",
       " '√≥': 265,\n",
       " '√¥': 266,\n",
       " '√µ': 267,\n",
       " '√∂': 268,\n",
       " '√∑': 269,\n",
       " '√∏': 270,\n",
       " '√π': 271,\n",
       " '√∫': 272,\n",
       " '√ª': 273,\n",
       " '√º': 274,\n",
       " '√Ω': 275,\n",
       " '√æ': 276,\n",
       " '√ø': 277,\n",
       " 'ƒÄ': 278,\n",
       " 'ƒÅ': 279,\n",
       " 'ƒÉ': 280,\n",
       " 'ƒÖ': 281,\n",
       " 'ƒÜ': 282,\n",
       " 'ƒá': 283,\n",
       " 'ƒå': 284,\n",
       " 'ƒç': 285,\n",
       " 'ƒè': 286,\n",
       " 'ƒê': 287,\n",
       " 'ƒë': 288,\n",
       " 'ƒì': 289,\n",
       " 'ƒó': 290,\n",
       " 'ƒô': 291,\n",
       " 'ƒõ': 292,\n",
       " 'ƒü': 293,\n",
       " 'ƒ°': 294,\n",
       " 'ƒ¶': 295,\n",
       " 'ƒß': 296,\n",
       " 'ƒ©': 297,\n",
       " 'ƒ™': 298,\n",
       " 'ƒ´': 299,\n",
       " 'ƒ∞': 300,\n",
       " 'ƒ±': 301,\n",
       " 'ƒº': 302,\n",
       " 'ƒΩ': 303,\n",
       " 'ƒæ': 304,\n",
       " '≈Å': 305,\n",
       " '≈Ç': 306,\n",
       " '≈Ñ': 307,\n",
       " '≈Ü': 308,\n",
       " '≈à': 309,\n",
       " '≈ã': 310,\n",
       " '≈å': 311,\n",
       " '≈ç': 312,\n",
       " '≈è': 313,\n",
       " '≈ë': 314,\n",
       " '≈í': 315,\n",
       " '≈ì': 316,\n",
       " '≈ô': 317,\n",
       " '≈ö': 318,\n",
       " '≈õ': 319,\n",
       " '≈û': 320,\n",
       " '≈ü': 321,\n",
       " '≈†': 322,\n",
       " '≈°': 323,\n",
       " '≈¢': 324,\n",
       " '≈£': 325,\n",
       " '≈•': 326,\n",
       " '≈©': 327,\n",
       " '≈´': 328,\n",
       " '≈≠': 329,\n",
       " '≈Ø': 330,\n",
       " '≈±': 331,\n",
       " '≈≥': 332,\n",
       " '≈µ': 333,\n",
       " '≈∑': 334,\n",
       " '≈∫': 335,\n",
       " '≈ª': 336,\n",
       " '≈º': 337,\n",
       " '≈Ω': 338,\n",
       " '≈æ': 339,\n",
       " '∆è': 340,\n",
       " '∆í': 341,\n",
       " '∆°': 342,\n",
       " '∆∞': 343,\n",
       " '«é': 344,\n",
       " '«ê': 345,\n",
       " '«í': 346,\n",
       " '«î': 347,\n",
       " '«´': 348,\n",
       " '»ò': 349,\n",
       " '»ô': 350,\n",
       " '»ö': 351,\n",
       " '»õ': 352,\n",
       " '…ê': 353,\n",
       " '…ë': 354,\n",
       " '…î': 355,\n",
       " '…ï': 356,\n",
       " '…ô': 357,\n",
       " '…õ': 358,\n",
       " '…°': 359,\n",
       " '…£': 360,\n",
       " '…®': 361,\n",
       " '…™': 362,\n",
       " '…≤': 363,\n",
       " '…æ': 364,\n",
       " ' Ä': 365,\n",
       " ' Å': 366,\n",
       " ' Ç': 367,\n",
       " ' É': 368,\n",
       " ' ä': 369,\n",
       " ' ã': 370,\n",
       " ' å': 371,\n",
       " ' ê': 372,\n",
       " ' ë': 373,\n",
       " ' í': 374,\n",
       " ' î': 375,\n",
       " ' ∞': 376,\n",
       " ' ≤': 377,\n",
       " ' ≥': 378,\n",
       " ' ∑': 379,\n",
       " ' ª': 380,\n",
       " ' º': 381,\n",
       " ' æ': 382,\n",
       " ' ø': 383,\n",
       " 'Àà': 384,\n",
       " 'Àê': 385,\n",
       " 'À°': 386,\n",
       " 'À¢': 387,\n",
       " 'À£': 388,\n",
       " 'ÃÅ': 389,\n",
       " 'ÃÉ': 390,\n",
       " 'Ãç': 391,\n",
       " 'ÃØ': 392,\n",
       " 'Õ°': 393,\n",
       " 'Œë': 394,\n",
       " 'Œí': 395,\n",
       " 'Œì': 396,\n",
       " 'Œî': 397,\n",
       " 'Œï': 398,\n",
       " 'Œó': 399,\n",
       " 'Œò': 400,\n",
       " 'Œô': 401,\n",
       " 'Œö': 402,\n",
       " 'Œõ': 403,\n",
       " 'Œú': 404,\n",
       " 'Œù': 405,\n",
       " 'Œü': 406,\n",
       " 'Œ†': 407,\n",
       " 'Œ£': 408,\n",
       " 'Œ§': 409,\n",
       " 'Œ¶': 410,\n",
       " 'Œß': 411,\n",
       " 'Œ®': 412,\n",
       " 'Œ©': 413,\n",
       " 'Œ¨': 414,\n",
       " 'Œ≠': 415,\n",
       " 'ŒÆ': 416,\n",
       " 'ŒØ': 417,\n",
       " 'Œ±': 418,\n",
       " 'Œ≤': 419,\n",
       " 'Œ≥': 420,\n",
       " 'Œ¥': 421,\n",
       " 'Œµ': 422,\n",
       " 'Œ∂': 423,\n",
       " 'Œ∑': 424,\n",
       " 'Œ∏': 425,\n",
       " 'Œπ': 426,\n",
       " 'Œ∫': 427,\n",
       " 'Œª': 428,\n",
       " 'Œº': 429,\n",
       " 'ŒΩ': 430,\n",
       " 'Œæ': 431,\n",
       " 'Œø': 432,\n",
       " 'œÄ': 433,\n",
       " 'œÅ': 434,\n",
       " 'œÇ': 435,\n",
       " 'œÉ': 436,\n",
       " 'œÑ': 437,\n",
       " 'œÖ': 438,\n",
       " 'œÜ': 439,\n",
       " 'œá': 440,\n",
       " 'œà': 441,\n",
       " 'œâ': 442,\n",
       " 'œå': 443,\n",
       " 'œç': 444,\n",
       " 'œé': 445,\n",
       " '–Ü': 446,\n",
       " '–à': 447,\n",
       " '–ê': 448,\n",
       " '–ë': 449,\n",
       " '–í': 450,\n",
       " '–ì': 451,\n",
       " '–î': 452,\n",
       " '–ï': 453,\n",
       " '–ñ': 454,\n",
       " '–ó': 455,\n",
       " '–ò': 456,\n",
       " '–ö': 457,\n",
       " '–õ': 458,\n",
       " '–ú': 459,\n",
       " '–ù': 460,\n",
       " '–û': 461,\n",
       " '–ü': 462,\n",
       " '–†': 463,\n",
       " '–°': 464,\n",
       " '–¢': 465,\n",
       " '–£': 466,\n",
       " '–§': 467,\n",
       " '–•': 468,\n",
       " '–¶': 469,\n",
       " '–ß': 470,\n",
       " '–®': 471,\n",
       " '–≠': 472,\n",
       " '–Æ': 473,\n",
       " '–Ø': 474,\n",
       " '–∞': 475,\n",
       " '–±': 476,\n",
       " '–≤': 477,\n",
       " '–≥': 478,\n",
       " '–¥': 479,\n",
       " '–µ': 480,\n",
       " '–∂': 481,\n",
       " '–∑': 482,\n",
       " '–∏': 483,\n",
       " '–π': 484,\n",
       " '–∫': 485,\n",
       " '–ª': 486,\n",
       " '–º': 487,\n",
       " '–Ω': 488,\n",
       " '–æ': 489,\n",
       " '–ø': 490,\n",
       " '—Ä': 491,\n",
       " '—Å': 492,\n",
       " '—Ç': 493,\n",
       " '—É': 494,\n",
       " '—Ñ': 495,\n",
       " '—Ö': 496,\n",
       " '—Ü': 497,\n",
       " '—á': 498,\n",
       " '—à': 499,\n",
       " '—â': 500,\n",
       " '—ä': 501,\n",
       " '—ã': 502,\n",
       " '—å': 503,\n",
       " '—ç': 504,\n",
       " '—é': 505,\n",
       " '—è': 506,\n",
       " '—ë': 507,\n",
       " '—ñ': 508,\n",
       " '—ó': 509,\n",
       " '—ò': 510,\n",
       " '—ö': 511,\n",
       " '—õ': 512,\n",
       " '‘±': 513,\n",
       " '’Ä': 514,\n",
       " '’°': 515,\n",
       " '’•': 516,\n",
       " '’´': 517,\n",
       " '’Ø': 518,\n",
       " '’¥': 519,\n",
       " '’µ': 520,\n",
       " '’∂': 521,\n",
       " '’∏': 522,\n",
       " '’Ω': 523,\n",
       " '’ø': 524,\n",
       " '÷Ä': 525,\n",
       " '÷Ç': 526,\n",
       " '÷∞': 527,\n",
       " '÷¥': 528,\n",
       " '÷µ': 529,\n",
       " '÷∂': 530,\n",
       " '÷∑': 531,\n",
       " '÷∏': 532,\n",
       " '÷π': 533,\n",
       " '÷º': 534,\n",
       " '◊ê': 535,\n",
       " '◊ë': 536,\n",
       " '◊í': 537,\n",
       " '◊ì': 538,\n",
       " '◊î': 539,\n",
       " '◊ï': 540,\n",
       " '◊ñ': 541,\n",
       " '◊ó': 542,\n",
       " '◊ò': 543,\n",
       " '◊ô': 544,\n",
       " '◊õ': 545,\n",
       " '◊ú': 546,\n",
       " '◊ù': 547,\n",
       " '◊û': 548,\n",
       " '◊ü': 549,\n",
       " '◊†': 550,\n",
       " '◊°': 551,\n",
       " '◊¢': 552,\n",
       " '◊§': 553,\n",
       " '◊¶': 554,\n",
       " '◊ß': 555,\n",
       " '◊®': 556,\n",
       " '◊©': 557,\n",
       " '◊™': 558,\n",
       " 'ÿå': 559,\n",
       " 'ÿ°': 560,\n",
       " 'ÿ¢': 561,\n",
       " 'ÿ£': 562,\n",
       " 'ÿ•': 563,\n",
       " 'ÿ¶': 564,\n",
       " 'ÿß': 565,\n",
       " 'ÿ®': 566,\n",
       " 'ÿ©': 567,\n",
       " 'ÿ™': 568,\n",
       " 'ÿ´': 569,\n",
       " 'ÿ¨': 570,\n",
       " 'ÿ≠': 571,\n",
       " 'ÿÆ': 572,\n",
       " 'ÿØ': 573,\n",
       " 'ÿ∞': 574,\n",
       " 'ÿ±': 575,\n",
       " 'ÿ≤': 576,\n",
       " 'ÿ≥': 577,\n",
       " 'ÿ¥': 578,\n",
       " 'ÿµ': 579,\n",
       " 'ÿ∂': 580,\n",
       " 'ÿ∑': 581,\n",
       " 'ÿ∏': 582,\n",
       " 'ÿπ': 583,\n",
       " 'ÿ∫': 584,\n",
       " 'ŸÅ': 585,\n",
       " 'ŸÇ': 586,\n",
       " 'ŸÉ': 587,\n",
       " 'ŸÑ': 588,\n",
       " 'ŸÖ': 589,\n",
       " 'ŸÜ': 590,\n",
       " 'Ÿá': 591,\n",
       " 'Ÿà': 592,\n",
       " 'Ÿâ': 593,\n",
       " 'Ÿä': 594,\n",
       " 'Ÿé': 595,\n",
       " 'Ÿê': 596,\n",
       " 'Ÿπ': 597,\n",
       " 'Ÿæ': 598,\n",
       " '⁄Ü': 599,\n",
       " '⁄©': 600,\n",
       " '⁄Ø': 601,\n",
       " '€Å': 602,\n",
       " '€å': 603,\n",
       " '€í': 604,\n",
       " '‡§Ç': 605,\n",
       " '‡§Ü': 606,\n",
       " '‡§ï': 607,\n",
       " '‡§ó': 608,\n",
       " '‡§ö': 609,\n",
       " '‡§ú': 610,\n",
       " '‡§£': 611,\n",
       " '‡§§': 612,\n",
       " '‡§¶': 613,\n",
       " '‡§ß': 614,\n",
       " '‡§®': 615,\n",
       " '‡§™': 616,\n",
       " '‡§¨': 617,\n",
       " '‡§≠': 618,\n",
       " '‡§Æ': 619,\n",
       " '‡§Ø': 620,\n",
       " '‡§∞': 621,\n",
       " '‡§≤': 622,\n",
       " '‡§µ': 623,\n",
       " '‡§∂': 624,\n",
       " '‡§∑': 625,\n",
       " '‡§∏': 626,\n",
       " '‡§π': 627,\n",
       " '‡§æ': 628,\n",
       " '‡§ø': 629,\n",
       " '‡•Ä': 630,\n",
       " '‡•Å': 631,\n",
       " '‡•á': 632,\n",
       " '‡•ã': 633,\n",
       " '‡•ç': 634,\n",
       " '‡•§': 635,\n",
       " '‡••': 636,\n",
       " '‡¶Ü': 637,\n",
       " '‡¶á': 638,\n",
       " '‡¶è': 639,\n",
       " '‡¶ì': 640,\n",
       " '‡¶ï': 641,\n",
       " '‡¶ñ': 642,\n",
       " '‡¶ó': 643,\n",
       " '‡¶ö': 644,\n",
       " '‡¶õ': 645,\n",
       " '‡¶ú': 646,\n",
       " '‡¶ü': 647,\n",
       " '‡¶§': 648,\n",
       " '‡¶•': 649,\n",
       " '‡¶¶': 650,\n",
       " '‡¶ß': 651,\n",
       " '‡¶®': 652,\n",
       " '‡¶™': 653,\n",
       " '‡¶¨': 654,\n",
       " '‡¶Æ': 655,\n",
       " '‡¶Ø': 656,\n",
       " '‡¶∞': 657,\n",
       " '‡¶≤': 658,\n",
       " '‡¶∂': 659,\n",
       " '‡¶∏': 660,\n",
       " '‡¶π': 661,\n",
       " '‡¶º': 662,\n",
       " '‡¶æ': 663,\n",
       " '‡¶ø': 664,\n",
       " '‡ßÄ': 665,\n",
       " '‡ßÅ': 666,\n",
       " '‡ßá': 667,\n",
       " '‡ßã': 668,\n",
       " '‡ßç': 669,\n",
       " '‡ßü': 670,\n",
       " '‡Æï': 671,\n",
       " '‡Æ§': 672,\n",
       " '‡Æ™': 673,\n",
       " '‡ÆÆ': 674,\n",
       " '‡ÆØ': 675,\n",
       " '‡Æ∞': 676,\n",
       " '‡Æ≤': 677,\n",
       " '‡Æµ': 678,\n",
       " '‡Ææ': 679,\n",
       " '‡Æø': 680,\n",
       " '‡ØÅ': 681,\n",
       " '‡Øç': 682,\n",
       " '‡∏£': 683,\n",
       " '‡ºã': 684,\n",
       " '‡ΩÇ': 685,\n",
       " '‡ΩÑ': 686,\n",
       " '‡Ωë': 687,\n",
       " '‡Ωì': 688,\n",
       " '‡Ωñ': 689,\n",
       " '‡Ωò': 690,\n",
       " '‡Ω¢': 691,\n",
       " '‡Ω£': 692,\n",
       " '‡Ω¶': 693,\n",
       " '‡Ω≤': 694,\n",
       " '‡Ω¥': 695,\n",
       " '‡Ω∫': 696,\n",
       " '‡Ωº': 697,\n",
       " '·Éê': 698,\n",
       " '·Éî': 699,\n",
       " '·Éò': 700,\n",
       " '·Éö': 701,\n",
       " '·Éú': 702,\n",
       " '·Éù': 703,\n",
       " '·É†': 704,\n",
       " '·É°': 705,\n",
       " '·¥¨': 706,\n",
       " '·¥µ': 707,\n",
       " '·µÄ': 708,\n",
       " '·µÉ': 709,\n",
       " '·µá': 710,\n",
       " '·µà': 711,\n",
       " '·µâ': 712,\n",
       " '·µç': 713,\n",
       " '·µè': 714,\n",
       " '·µê': 715,\n",
       " '·µí': 716,\n",
       " '·µñ': 717,\n",
       " '·µó': 718,\n",
       " '·µò': 719,\n",
       " '·µ¢': 720,\n",
       " '·µ£': 721,\n",
       " '·µ§': 722,\n",
       " '·µ•': 723,\n",
       " '·∂ú': 724,\n",
       " '·∂†': 725,\n",
       " '·∏ç': 726,\n",
       " '·∏§': 727,\n",
       " '·∏•': 728,\n",
       " '·∏®': 729,\n",
       " '·∏©': 730,\n",
       " '·∏≥': 731,\n",
       " '·πÉ': 732,\n",
       " '·πÖ': 733,\n",
       " '·πá': 734,\n",
       " '·πõ': 735,\n",
       " '·π£': 736,\n",
       " '·π≠': 737,\n",
       " '·∫°': 738,\n",
       " '·∫£': 739,\n",
       " '·∫•': 740,\n",
       " '·∫ß': 741,\n",
       " '·∫©': 742,\n",
       " '·∫≠': 743,\n",
       " '·∫Ø': 744,\n",
       " '·∫ø': 745,\n",
       " '·ªÅ': 746,\n",
       " '·ªÉ': 747,\n",
       " '·ªÖ': 748,\n",
       " '·ªá': 749,\n",
       " '·ªã': 750,\n",
       " '·ªç': 751,\n",
       " '·ªë': 752,\n",
       " '·ªì': 753,\n",
       " '·ªï': 754,\n",
       " '·ªô': 755,\n",
       " '·ªõ': 756,\n",
       " '·ªù': 757,\n",
       " '·ª£': 758,\n",
       " '·ª•': 759,\n",
       " '·ªß': 760,\n",
       " '·ª©': 761,\n",
       " '·ª´': 762,\n",
       " '·ª≠': 763,\n",
       " '·ªØ': 764,\n",
       " '·ª±': 765,\n",
       " '·ª≥': 766,\n",
       " '·ªπ': 767,\n",
       " '·ºÄ': 768,\n",
       " '·ºê': 769,\n",
       " '·ΩÅ': 770,\n",
       " '·Ωê': 771,\n",
       " '·Ω∞': 772,\n",
       " '·Ω∂': 773,\n",
       " '·Ω∏': 774,\n",
       " '·øÜ': 775,\n",
       " '·øñ': 776,\n",
       " '·ø¶': 777,\n",
       " '·ø∂': 778,\n",
       " '‚Äê': 779,\n",
       " '‚Äë': 780,\n",
       " '‚Äí': 781,\n",
       " '‚Äì': 782,\n",
       " '‚Äî': 783,\n",
       " '‚Äï': 784,\n",
       " '‚Äñ': 785,\n",
       " '‚Äò': 786,\n",
       " '‚Äô': 787,\n",
       " '‚Äö': 788,\n",
       " '‚Äú': 789,\n",
       " '‚Äù': 790,\n",
       " '‚Äû': 791,\n",
       " '‚Ä†': 792,\n",
       " '‚Ä°': 793,\n",
       " '‚Ä¢': 794,\n",
       " '‚Ä¶': 795,\n",
       " '‚Ä∞': 796,\n",
       " '‚Ä≤': 797,\n",
       " '‚Ä≥': 798,\n",
       " '‚ÅÑ': 799,\n",
       " '‚Å∞': 800,\n",
       " '‚Å±': 801,\n",
       " '‚Å¥': 802,\n",
       " '‚Åµ': 803,\n",
       " '‚Å∂': 804,\n",
       " '‚Å∑': 805,\n",
       " '‚Å∏': 806,\n",
       " '‚Åπ': 807,\n",
       " '‚Å∫': 808,\n",
       " '‚Åª': 809,\n",
       " '‚Åø': 810,\n",
       " '‚ÇÄ': 811,\n",
       " '‚ÇÅ': 812,\n",
       " '‚ÇÇ': 813,\n",
       " '‚ÇÉ': 814,\n",
       " '‚ÇÑ': 815,\n",
       " '‚ÇÖ': 816,\n",
       " '‚ÇÜ': 817,\n",
       " '‚Çá': 818,\n",
       " '‚Çà': 819,\n",
       " '‚Çâ': 820,\n",
       " '‚Çä': 821,\n",
       " '‚Çç': 822,\n",
       " '‚Çé': 823,\n",
       " '‚Çê': 824,\n",
       " '‚Çë': 825,\n",
       " '‚Çí': 826,\n",
       " '‚Çì': 827,\n",
       " '‚Çï': 828,\n",
       " '‚Çñ': 829,\n",
       " '‚Çò': 830,\n",
       " '‚Çô': 831,\n",
       " '‚Çö': 832,\n",
       " '‚Çõ': 833,\n",
       " '‚Çú': 834,\n",
       " '‚Ç§': 835,\n",
       " '‚Ç¨': 836,\n",
       " '‚Ç±': 837,\n",
       " '‚Çπ': 838,\n",
       " '‚Ñì': 839,\n",
       " '‚Ññ': 840,\n",
       " '‚Ñù': 841,\n",
       " '‚Öì': 842,\n",
       " '‚Üê': 843,\n",
       " '‚Üë': 844,\n",
       " '‚Üí': 845,\n",
       " '‚Üî': 846,\n",
       " '‚áå': 847,\n",
       " '‚áí': 848,\n",
       " '‚àÇ': 849,\n",
       " '‚àà': 850,\n",
       " '‚àí': 851,\n",
       " '‚àó': 852,\n",
       " '‚àò': 853,\n",
       " '‚àö': 854,\n",
       " '‚àû': 855,\n",
       " '‚àß': 856,\n",
       " '‚à®': 857,\n",
       " '‚à©': 858,\n",
       " '‚à™': 859,\n",
       " '‚âà': 860,\n",
       " '‚â†': 861,\n",
       " '‚â°': 862,\n",
       " '‚â§': 863,\n",
       " '‚â•': 864,\n",
       " '‚äÇ': 865,\n",
       " '‚äÜ': 866,\n",
       " '‚äï': 867,\n",
       " '‚ãÖ': 868,\n",
       " '‚îÄ': 869,\n",
       " '‚îÇ': 870,\n",
       " '‚ñ†': 871,\n",
       " '‚óè': 872,\n",
       " '‚òÖ': 873,\n",
       " '‚òÜ': 874,\n",
       " '‚òâ': 875,\n",
       " '‚ô†': 876,\n",
       " '‚ô£': 877,\n",
       " '‚ô•': 878,\n",
       " '‚ô¶': 879,\n",
       " '‚ô≠': 880,\n",
       " '‚ôØ': 881,\n",
       " '‚ü®': 882,\n",
       " '‚ü©': 883,\n",
       " '‚±º': 884,\n",
       " '„ÄÅ': 885,\n",
       " '„ÄÇ': 886,\n",
       " '„Ää': 887,\n",
       " '„Äã': 888,\n",
       " '„Äå': 889,\n",
       " '„Äç': 890,\n",
       " '„Äé': 891,\n",
       " '„Äè': 892,\n",
       " '„Äú': 893,\n",
       " '„ÅÑ': 894,\n",
       " '„ÅÜ': 895,\n",
       " '„Åà': 896,\n",
       " '„Åä': 897,\n",
       " '„Åã': 898,\n",
       " '„Åç': 899,\n",
       " '„Åè': 900,\n",
       " '„Åë': 901,\n",
       " '„Åì': 902,\n",
       " '„Åï': 903,\n",
       " '„Åó': 904,\n",
       " '„Åô': 905,\n",
       " '„Åõ': 906,\n",
       " '„Åù': 907,\n",
       " '„Åü': 908,\n",
       " '„Å°': 909,\n",
       " '„Å§': 910,\n",
       " '„Å¶': 911,\n",
       " '„Å®': 912,\n",
       " '„Å™': 913,\n",
       " '„Å´': 914,\n",
       " '„ÅÆ': 915,\n",
       " '„ÅØ': 916,\n",
       " '„Å≤': 917,\n",
       " '„Åæ': 918,\n",
       " '„Åø': 919,\n",
       " '„ÇÄ': 920,\n",
       " '„ÇÅ': 921,\n",
       " '„ÇÇ': 922,\n",
       " '„ÇÑ': 923,\n",
       " '„ÇÜ': 924,\n",
       " '„Çà': 925,\n",
       " '„Çâ': 926,\n",
       " '„Çä': 927,\n",
       " '„Çã': 928,\n",
       " '„Çå': 929,\n",
       " '„Çì': 930,\n",
       " '„Ç¢': 931,\n",
       " '„Ç£': 932,\n",
       " '„Ç§': 933,\n",
       " '„Ç¶': 934,\n",
       " '„Ç®': 935,\n",
       " '„Ç™': 936,\n",
       " '„Ç´': 937,\n",
       " '„Ç¨': 938,\n",
       " '„Ç≠': 939,\n",
       " '„ÇØ': 940,\n",
       " '„Ç∞': 941,\n",
       " '„Ç≥': 942,\n",
       " '„Çµ': 943,\n",
       " '„Ç∑': 944,\n",
       " '„Ç∏': 945,\n",
       " '„Çπ': 946,\n",
       " '„Ç∫': 947,\n",
       " '„Çø': 948,\n",
       " '„ÉÄ': 949,\n",
       " '„ÉÉ': 950,\n",
       " '„ÉÜ': 951,\n",
       " '„Éá': 952,\n",
       " '„Éà': 953,\n",
       " '„Éâ': 954,\n",
       " '„Éä': 955,\n",
       " '„Éã': 956,\n",
       " '„Éè': 957,\n",
       " '„Éê': 958,\n",
       " '„Éë': 959,\n",
       " '„Éï': 960,\n",
       " '„Éñ': 961,\n",
       " '„Éó': 962,\n",
       " '„Éû': 963,\n",
       " '„Éü': 964,\n",
       " '„É†': 965,\n",
       " '„É£': 966,\n",
       " '„É•': 967,\n",
       " '„É©': 968,\n",
       " '„É™': 969,\n",
       " '„É´': 970,\n",
       " '„É¨': 971,\n",
       " '„É≠': 972,\n",
       " '„É≥': 973,\n",
       " '„Éª': 974,\n",
       " '„Éº': 975,\n",
       " '‰∏Ä': 976,\n",
       " '‰∏â': 977,\n",
       " '‰∏ä': 978,\n",
       " '‰∏ã': 979,\n",
       " '‰∏≠': 980,\n",
       " '‰∫ã': 981,\n",
       " '‰∫å': 982,\n",
       " '‰∫ï': 983,\n",
       " '‰∫¨': 984,\n",
       " '‰∫∫': 985,\n",
       " '‰∫ª': 986,\n",
       " '‰ªÅ': 987,\n",
       " '‰Ωê': 988,\n",
       " '‰æç': 989,\n",
       " 'ÂÖâ': 990,\n",
       " 'ÂÖ¨': 991,\n",
       " 'Âäõ': 992,\n",
       " 'Âåó': 993,\n",
       " 'ÂçÅ': 994,\n",
       " 'Âçó': 995,\n",
       " 'Âéü': 996,\n",
       " 'Âè£': 997,\n",
       " 'Âè≤': 998,\n",
       " 'Âè∏': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "wordpiece_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(len(wordpiece_tokenizer.get_vocab()))\n",
    "wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', 'aren', \"'\", 't', 'special', 'enough', 'to', 'break', 'the', 'rules', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpiece_tokenizer.tokenize(python_zen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word', 'piece</w>']\n",
      "['word', '##piece']\n"
     ]
    }
   ],
   "source": [
    "print(bpe_tokenizer.tokenize('wordpiece'))\n",
    "print(wordpiece_tokenizer.tokenize('wordpiece'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m.</w>', 'night</w>', 'shy', 'am', 'alan</w>']\n",
      "['M', '.', 'Night', 'S', '##hya', '##mal', '##an']\n"
     ]
    }
   ],
   "source": [
    "# Like directors\n",
    "print(bpe_tokenizer.tokenize('M. Night Shyamalan'))\n",
    "print(wordpiece_tokenizer.tokenize('M. Night Shyamalan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':</w>', 'hippo', 'po', 'tom', 'on', 'stro', 'se', 'squi', 'pp', 'ed', 'ali', 'op', 'ho', 'bia</w>']\n",
      "[':', 'Hip', '##pop', '##oto', '##mons', '##tro', '##ses', '##qui', '##pped', '##ali', '##op', '##ho', '##bia']\n"
     ]
    }
   ],
   "source": [
    "# The fear of long words\n",
    "print(bpe_tokenizer.tokenize(\":Hippopotomonstrosesquippedaliophobia\"))\n",
    "print(wordpiece_tokenizer.tokenize(\":Hippopotomonstrosesquippedaliophobia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['projec', 't', 'contrac', 't', 'char', 'ging', 'peri', 'od', 'projec', 't', 'accoun', 'tre', 'fer', 'en', 'c', 'ev', 'm</w>']\n",
      "['Project', '##C', '##ont', '##rac', '##t', '##C', '##har', '##ging', '##P', '##eri', '##od', '##P', '##ro', '##ject', '##A', '##cco', '##unt', '##R', '##ef', '##ere', '##nce', '##V', '##M']\n"
     ]
    }
   ],
   "source": [
    "# A very long Java Class name\n",
    "print(bpe_tokenizer.tokenize('ProjectContractChargingPeriodProjectAccountReferenceVM'))  \n",
    "print(wordpiece_tokenizer.tokenize('ProjectContractChargingPeriodProjectAccountReferenceVM'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unigram\n",
    "- Takes a opposite approach as BPE and WordPiece: starts with many long words and breaks them up. \n",
    "- Tries to remove words with the least impact on the loss (i.e. are least required). \n",
    "- Unigram is often used through SentencePiece\n",
    "- Used in SOTA models such as: AlBERT, T5, mBART, Big Bird, and XLNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take the same corpus again:\n",
    "```yaml\n",
    "corpus: (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "vocabulary: [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"]\n",
    "```\n",
    "The vocab is all possible subtokens, that aren't spanning the whole word. \n",
    "\n",
    "We see `hug` is in here, due to `hugs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The unigram model sees each token independently of any token that came prior. The probability of each token given the previous context is just the probability of that token. \n",
    "The probability of a token is calculated over the total vocabulary:\n",
    "$$ \\mathcal{P}(t) = \\frac{f_t}{\\sum_{t \\in T}f_t} $$\n",
    "\n",
    "Where $T$ is the set of tokens and $f_t$ indicates the frequency of a token. \n",
    "\n",
    "So pretty simple: if the token `ug` is present 20 times, and the sum of all token frequencies is 210, we get: \n",
    "\n",
    "$$ \\mathcal{P}(\\text{\"ug\"}) = \\frac{20}{210} = 0.095 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```yaml\n",
    "corpus: (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "vocab: (\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16)\n",
    "(\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5)\n",
    "```\n",
    "\n",
    "These are the frequencies for each token in our vocabulary. The frequency sum is 210. \n",
    "\n",
    "If we want to calculate the probability of a word, we just multiple the probabilities of each token. \n",
    "\n",
    "$$ \\mathcal{P}([\\text{\"p\"}, \\text{\"u\"}, \\text{\"g\"}]) = \\mathcal{P}(\\text{\"p\"}) \\times \\mathcal{P}(\\text{\"u\"}) \\times \\mathcal{P}(\\text{\"g\"}) = \\frac{17}{210} \\times \\frac{36}{210} \\times \\frac{20}{210} = 0.0013 $$\n",
    "\n",
    "For each word, the chosen representation is the one with the highest probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we train then? \n",
    "The model is essentially overfitted to the dataset. \n",
    "1. Calculate the loss for the whole corpus, with the current vocabulary. \n",
    "2. Remove the vocabulary token whose removal increases the loss the least. \n",
    "3. We continue removing tokens until we reach our desired vocabulary size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Loss for one word is calculated as:\n",
    "\n",
    "$$ \\text{loss}_w = f_w \\cdot -\\log \\mathcal{P}(w)  $$\n",
    "So taking `pug` as an example:\n",
    "$$\n",
    " \\text{loss}_{\\text{\"pug\"}} = 5 \\cdot -\\log0.0013 \\\\ \n",
    " = 5 \\cdot 2.88 \\\\\n",
    " = 14.43  $$\n",
    " \n",
    " Total loss is then calculated with:\n",
    " $$\\text{loss} = \\sum_{w \\in W} \\text{loss}_w $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algorithm is as follows:\n",
    "1. Calculate loss for each word in the corpus with current vocab and sum.\n",
    "2. Iteratively remove a token from the vocabulary.\n",
    "3. Calculate loss with new vocabulary.\n",
    "4. Remove the token with the lowest negative effect on the loss.\n",
    "5. If vocab size > desired vocab size: go back to step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# And that's how you train a unigram tokenizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In conclusion\n",
    "\n",
    "3 types of tokenizers:\n",
    "- Wordbased\n",
    "- Character-based\n",
    "- Subword-based \n",
    "\n",
    "Where Subword tokenization has three different methods:\n",
    "- BPE - Pair frequency based\n",
    "- WordPiece/SentencePiece - Pair frequency vs token frequency based\n",
    "- Unigram - Loss minimization"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "c38f211982bd679d5ca5d79a71d7517c654455bae4af5c30a3c557803a2cce89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
