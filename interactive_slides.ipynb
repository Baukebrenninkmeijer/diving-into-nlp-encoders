{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Based Tokenizer\n",
    "\n",
    "- Split on some delimiter\n",
    "- Most basically: split on whitespace.\n",
    "- Split on punctuation and more tokens for more advanced behaviour. \n",
    "\n",
    "Depending on your delimiter, you will get different tokens with different meanings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', \"aren't\", 'special', 'enough', 'to', 'break', 'the', 'rules.']\n"
     ]
    }
   ],
   "source": [
    "python_zen = \"Special cases aren't special enough to break the rules.\"\n",
    "tokens = python_zen.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now create IDs from this as well. For this, we create a mapping between our tokens and IDs, which we call the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Special': 0, 'cases': 1, \"aren't\": 2, 'special': 3, 'enough': 4, 'to': 5, 'break': 6, 'the': 7, 'rules.': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab = {y: x for x, y in enumerate(tokens)}\n",
    "print(vocab)  # We can show the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['special']  # We can map tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!-- ![obama](images/obama.gif) -->\n",
    "<img src=\"images/obama.gif\" style=\"margin-left:auto;margin-right:auto;height:500px%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a closer look at our tokens\n",
    "\n",
    "```python\n",
    "['Special', 'cases', \"aren't\", 'special', 'enough', 'to', 'break', 'the', 'rules.']\n",
    "```\n",
    "\n",
    "We see `rules.` has a dot in its token. So the token `rules` will be different from `rules.`. Maybe that's not super optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**We should probably split on punctuation as well**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting on punctuation and more\n",
    "We can use `NLTK` (Natural Language Toolkit), a super useful NLP package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', 'aren', \"'\", 't', 'special', 'enough', 'to', 'break', 'the', 'rules', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize  # Simple RegEx based tokenizer\n",
    "print(wordpunct_tokenize(python_zen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Even more smartly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/baukebrenninkmeijer/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', 'are', \"n't\", 'special', 'enough', 'to', 'break', 'the', 'rules', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  # Requires a download. Uses improved TreebankWordTokenizer + tricks.\n",
    "print(word_tokenize(python_zen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `are` and `aren't` are still fairly simple, since `n't` is typically appended to negate something. Part of standard contractions in English. \n",
    "- Having a rule to handle `n't` works well enough. (e.g., \n",
    "```python\n",
    ".replace(\"n't\", \" n't\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, this doesn't hold for other combined words. For example:\n",
    "\n",
    "- `token` \n",
    "- `tokens`\n",
    "- `tokenizer`\n",
    "- `tokenization` \n",
    "\n",
    "all are unique tokens and get corresponding unique IDs. \n",
    "\n",
    "This result in **a lot** of tokens to be able to understand our text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common fix is to limit the vocabulary size. Typically, the $n$ most frequent words will be allowed in the vocab. \n",
    "\n",
    "Will render a lot of words **Out of Vocabulary (OOV)**!\n",
    "\n",
    "OOV is typically tokenized with the `<UNK>` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Usage of word tokenizer\n",
    "\n",
    "**Word-based tokenizers** were really popular before the NLP revolution in 2017. \n",
    "\n",
    "Both **GloVe** and **Word2Vec** used word-based tokenization. Focus was much more on the embedding algorithm, rather than the tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As shown, word-based tokenizers have some downsides. \n",
    "\n",
    "## Let's see if we can fix them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Character Based Tokenizers\n",
    "These tokenizers are conceptually very simple. Each character is a token.\n",
    "\n",
    "- `Small vocabulary`. For ASCII, 256. With unicode, this is max 1.1M characters.\n",
    "- No *out of vocabulary* tokens.\n",
    "- Lose a lot of meaninful information\n",
    "- Models typically have `max input lengths` (e.g., 256 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'a', 't']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('Cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mapping to a vocabulary\n",
    "The vocabulary of a text will be really simple, with one index mapping to a letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 0,\n",
       " 'k': 1,\n",
       " 'g': 2,\n",
       " 'i': 3,\n",
       " 'S': 4,\n",
       " 'h': 5,\n",
       " ' ': 6,\n",
       " \"'\": 7,\n",
       " 'u': 8,\n",
       " 'r': 9,\n",
       " 't': 10,\n",
       " 's': 11,\n",
       " 'p': 12,\n",
       " 'a': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'c': 16,\n",
       " '.': 17,\n",
       " 'l': 18,\n",
       " 'e': 19}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{y:x for x, y in enumerate(set(python_zen))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Character based tokenization of Chinese characters\n",
    "*Disclaimer: I do not speak a word of chinese*\n",
    "\n",
    "- Characters have meaning by themselves\n",
    "- But can change by combining characters\n",
    "\n",
    "For example:\n",
    "\n",
    "[馬來西亞 means Malaysia, but taken separately they mean \"Horse come to Western Asia\".](https://medium.com/@jjsham/nlp-tokenizing-chinese-phases-3302da4336bf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some good reads that use character based tokenization:\n",
    "1. [Neural Machine Translation in Linear Time. Kalchbrenner et al. 2017](https://arxiv.org/pdf/1610.10099.pdf)\n",
    "2. [Fully Character-Level Neural Machine Translation without Explicit Segmentation - Lee et al. 2017.](https://aclanthology.org/Q17-1026.pdf)\n",
    "3. [Learning to Generate Reviews and Discovering Sentiment - Radford et al. 2017.](https://arxiv.org/pdf/1704.01444.pdf)\n",
    "\n",
    "From these papers, you can already see that these tokenizers were mainly used a while back, around 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sub-word tokenizers\n",
    "\n",
    "Subword tokenizers combine the two approaches of word and character tokenizers. \n",
    "Keep the advantages of both, limit the disadvantages.\n",
    "\n",
    "Types:\n",
    "- Byte-Pair encoding\n",
    "- WordPiece\n",
    "- Unigram\n",
    "\n",
    "Special case:\n",
    "- SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Byte Pair Encoding (BPE)\n",
    "\n",
    "- Based on a lossless data compression algorithm, proposed by Philip Gage in 1994.\n",
    "- Replace the most common pair of bytes with a byte that does not occur in the data.\n",
    "- First adapted for Neural Machine Translation by [*Sennrich et al.*](https://aclanthology.org/P16-1162.pdf) in 2015.\n",
    "- Used in many SOTA models: Transformer, GPT, GPT-2, XLM, FlauBERT, Roberta.\n",
    "\n",
    "\n",
    "Note: BPE requires the data be split into words already. This can be whitespace splitting, or more advanced like Spacy/NLTK. This step is called pre-tokenization. We'll see why this is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Example from wikipedia\n",
    "We'll encode:\n",
    "\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "The most common pair is `aa`, which will be replace by a byte in the data that is not present, say `Z`. \n",
    "\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We repeat this step, replacing `ab` with `Y`:\n",
    "\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The only byte-pair left is now `ac`, which only occurs once. However, you could continue recursively, replacing `ZY` with `X` if that should occur frequently.\n",
    "\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Adapting this to the NLP domain, the list of replacements (`Z`, `Y` and `X`) can now be considered your vocabulary with tokens, with the slight change that all tokens are initially in the vocabulary, and they are incrementally merged together. \n",
    "\n",
    "**Let's see if we have some time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A natural language example\n",
    "*Note: example taken from the excellent [Huggingface tokenizer summary](https://huggingface.co/docs/transformers/tokenizer_summary)* 🤗.\n",
    "\n",
    "Let's say that after pre-tokenization, we have the following set of words. With BPE, we need the words and their counts, because we want to merge based on frequency. \n",
    "```python\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "The base vocab consequently is: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Splitting all words into vocabulary symbols, we get:\n",
    "\n",
    "```python\n",
    "(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n",
    "```\n",
    "BPE counts the frequency of all symbol pairs, and picks the most frequent. Here, we have `u` + `g` present $10+5+5=20$ times. \n",
    "\n",
    "Consequently, `ug` is added to the vocabulary, and we replace the `u`+`g` symbols in our words with `ug`. This results in:\n",
    "```python\n",
    "(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n",
    "```\n",
    "\n",
    "And our vocab now is: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameter: `vocab_size`\n",
    "We continue iteratively applying this algorithms until we hit our desired vocabulary size, which is our vocab size + the number of merges. For example, GPT has a vocabulary of 40,478 build up from 478 base characters and 40.000 merges.\n",
    "\n",
    "This learned vocabulary can now be used for unseen text. However, consider that if unknown characters are encountered they will be encoded as `<UNK>` (e.g., `mug` would become `[\"<UNK>\", \"ug\"]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataset specific vocabulary\n",
    "A tokenizer trained on the common crawl will differ from one trained on Wikipedia. For example, GPT is trained on the BooksCorpus (800M words).\n",
    "\n",
    "**Let's have a look at the trained GPT tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 1,\n",
       " ',': 2,\n",
       " 't': 3,\n",
       " 'h': 4,\n",
       " 'e': 5,\n",
       " '\"': 6,\n",
       " 'o': 7,\n",
       " 'a': 8,\n",
       " 'n': 9,\n",
       " 'd': 10,\n",
       " 'i': 11,\n",
       " 'f': 12,\n",
       " 'w': 13,\n",
       " 's': 14,\n",
       " 'y': 15,\n",
       " 'u': 16,\n",
       " 'r': 17,\n",
       " \"'\": 18,\n",
       " '?': 19,\n",
       " 'm': 20,\n",
       " 'b': 21,\n",
       " '-': 22,\n",
       " 'v': 23,\n",
       " 'p': 24,\n",
       " 'c': 25,\n",
       " 'l': 26,\n",
       " 'k': 27,\n",
       " 'j': 28,\n",
       " '!': 29,\n",
       " 'g': 30,\n",
       " '*': 31,\n",
       " ';': 32,\n",
       " ':': 33,\n",
       " 'x': 34,\n",
       " 'q': 35,\n",
       " 'z': 36,\n",
       " ')': 37,\n",
       " '(': 38,\n",
       " '1': 39,\n",
       " '/': 40,\n",
       " '_': 41,\n",
       " '2': 42,\n",
       " '3': 43,\n",
       " '4': 44,\n",
       " '~': 45,\n",
       " '5': 46,\n",
       " '#': 47,\n",
       " '0': 48,\n",
       " '6': 49,\n",
       " '7': 50,\n",
       " '$': 51,\n",
       " '>': 52,\n",
       " '9': 53,\n",
       " '8': 54,\n",
       " '[': 55,\n",
       " ']': 56,\n",
       " '<': 57,\n",
       " '&': 58,\n",
       " '%': 59,\n",
       " '¨': 60,\n",
       " '`': 61,\n",
       " 'é': 62,\n",
       " '»': 63,\n",
       " '«': 64,\n",
       " '=': 65,\n",
       " '•': 66,\n",
       " '@': 67,\n",
       " '+': 68,\n",
       " '©': 69,\n",
       " '¡': 70,\n",
       " '{': 71,\n",
       " '}': 72,\n",
       " 'ª': 73,\n",
       " 'ñ': 74,\n",
       " 'ï': 75,\n",
       " '‖': 76,\n",
       " 'ç': 77,\n",
       " 'í': 78,\n",
       " '^': 79,\n",
       " '£': 80,\n",
       " '§': 81,\n",
       " '♥': 82,\n",
       " '−': 83,\n",
       " 'à': 84,\n",
       " '|': 85,\n",
       " '°': 86,\n",
       " '¦': 87,\n",
       " 'ł': 88,\n",
       " 'ĩ': 89,\n",
       " 'ü': 90,\n",
       " '®': 91,\n",
       " 'ù': 92,\n",
       " 'á': 93,\n",
       " 'â': 94,\n",
       " 'ó': 95,\n",
       " 'è': 96,\n",
       " '∞': 97,\n",
       " 'ë': 98,\n",
       " 'ä': 99,\n",
       " '♪': 100,\n",
       " 'ò': 101,\n",
       " 'ω': 102,\n",
       " '▪': 103,\n",
       " '½': 104,\n",
       " 'ǒ': 105,\n",
       " '‡': 106,\n",
       " 'ê': 107,\n",
       " '◊': 108,\n",
       " '►': 109,\n",
       " '۞': 110,\n",
       " 'ú': 111,\n",
       " '€': 112,\n",
       " 'æ': 113,\n",
       " 'î': 114,\n",
       " '↕': 115,\n",
       " 'ô': 116,\n",
       " 'ē': 117,\n",
       " 'ǐ': 118,\n",
       " '♫': 119,\n",
       " '�': 120,\n",
       " '\\uf04a': 121,\n",
       " '™': 122,\n",
       " 'ř': 123,\n",
       " 'ā': 124,\n",
       " '·': 125,\n",
       " '¿': 126,\n",
       " '\\\\': 127,\n",
       " '─': 128,\n",
       " '\\uf067': 129,\n",
       " '\\uf020': 130,\n",
       " '∙': 131,\n",
       " 'ॐ': 132,\n",
       " 'ö': 133,\n",
       " 'ø': 134,\n",
       " '\\uf06c': 135,\n",
       " '\\uf09b': 136,\n",
       " '●': 137,\n",
       " '\\xad': 138,\n",
       " '■': 139,\n",
       " '\\uf063': 140,\n",
       " '†': 141,\n",
       " 'å': 142,\n",
       " 'ō': 143,\n",
       " 'ã': 144,\n",
       " '¤': 145,\n",
       " '⠔': 146,\n",
       " '\\uf059': 147,\n",
       " 'н': 148,\n",
       " '\\uf09a': 149,\n",
       " '⚔': 150,\n",
       " 'ă': 151,\n",
       " 'û': 152,\n",
       " 'º': 153,\n",
       " '♦': 154,\n",
       " 'ĝ': 155,\n",
       " '¹': 156,\n",
       " '═': 157,\n",
       " '\\uf0a3': 158,\n",
       " '¾': 159,\n",
       " 'ì': 160,\n",
       " '☼': 161,\n",
       " 'ș': 162,\n",
       " '¼': 163,\n",
       " '☺': 164,\n",
       " 'đ': 165,\n",
       " 'ą': 166,\n",
       " 'ǽ': 167,\n",
       " '╦': 168,\n",
       " '\\uf02a': 169,\n",
       " '¬': 170,\n",
       " 'ī': 171,\n",
       " '\\u200b': 172,\n",
       " 'œ': 173,\n",
       " '¢': 174,\n",
       " 'ǎ': 175,\n",
       " 'š': 176,\n",
       " 'ʻ': 177,\n",
       " 'ν': 178,\n",
       " 'α': 179,\n",
       " '\\uf05d': 180,\n",
       " 'я': 181,\n",
       " 'б': 182,\n",
       " 'й': 183,\n",
       " 'τ': 184,\n",
       " 'ο': 185,\n",
       " 'ε': 186,\n",
       " 'ί': 187,\n",
       " 'ι': 188,\n",
       " 'δ': 189,\n",
       " '\\uf073': 190,\n",
       " '\\uf05e': 191,\n",
       " '‐': 192,\n",
       " 'с': 193,\n",
       " '\\uf0a5': 194,\n",
       " 'þ': 195,\n",
       " 'κ': 196,\n",
       " '‑': 197,\n",
       " '‒': 198,\n",
       " 'к': 199,\n",
       " '\\uf07e': 200,\n",
       " 'σ': 201,\n",
       " 'č': 202,\n",
       " '\\uf0be': 203,\n",
       " 'υ': 204,\n",
       " 'ό': 205,\n",
       " '\\uf061': 206,\n",
       " '\\uf02d': 207,\n",
       " 'ß': 208,\n",
       " 'е': 209,\n",
       " 'т': 210,\n",
       " 'μ': 211,\n",
       " 'π': 212,\n",
       " 'ρ': 213,\n",
       " 'έ': 214,\n",
       " 'в': 215,\n",
       " 'ş': 216,\n",
       " 'о': 217,\n",
       " 'ð': 218,\n",
       " 'а': 219,\n",
       " 'ς': 220,\n",
       " 'р': 221,\n",
       " 'м': 222,\n",
       " 'у': 223,\n",
       " 'ή': 224,\n",
       " 'ά': 225,\n",
       " 'и': 226,\n",
       " 'д': 227,\n",
       " '\\uf0bc': 228,\n",
       " '\\uf070': 229,\n",
       " 'λ': 230,\n",
       " 'л': 231,\n",
       " 'γ': 232,\n",
       " '¯': 233,\n",
       " '\\uf065': 234,\n",
       " '\\uf043': 235,\n",
       " '\\uf074': 236,\n",
       " '\\uf068': 237,\n",
       " '\\uf072': 238,\n",
       " '.</w>': 239,\n",
       " ',</w>': 240,\n",
       " 't</w>': 241,\n",
       " 'h</w>': 242,\n",
       " 'e</w>': 243,\n",
       " '\"</w>': 244,\n",
       " 'o</w>': 245,\n",
       " 'a</w>': 246,\n",
       " 'n</w>': 247,\n",
       " 'd</w>': 248,\n",
       " 'i</w>': 249,\n",
       " 'f</w>': 250,\n",
       " 'w</w>': 251,\n",
       " 's</w>': 252,\n",
       " 'y</w>': 253,\n",
       " 'u</w>': 254,\n",
       " 'r</w>': 255,\n",
       " \"'</w>\": 256,\n",
       " '?</w>': 257,\n",
       " 'm</w>': 258,\n",
       " 'b</w>': 259,\n",
       " '-</w>': 260,\n",
       " 'v</w>': 261,\n",
       " 'p</w>': 262,\n",
       " 'c</w>': 263,\n",
       " 'l</w>': 264,\n",
       " 'k</w>': 265,\n",
       " 'j</w>': 266,\n",
       " '!</w>': 267,\n",
       " 'g</w>': 268,\n",
       " '*</w>': 269,\n",
       " ';</w>': 270,\n",
       " ':</w>': 271,\n",
       " 'x</w>': 272,\n",
       " 'q</w>': 273,\n",
       " 'z</w>': 274,\n",
       " ')</w>': 275,\n",
       " '(</w>': 276,\n",
       " '1</w>': 277,\n",
       " '/</w>': 278,\n",
       " '_</w>': 279,\n",
       " '2</w>': 280,\n",
       " '3</w>': 281,\n",
       " '4</w>': 282,\n",
       " '~</w>': 283,\n",
       " '5</w>': 284,\n",
       " '#</w>': 285,\n",
       " '0</w>': 286,\n",
       " '6</w>': 287,\n",
       " '7</w>': 288,\n",
       " '$</w>': 289,\n",
       " '></w>': 290,\n",
       " '9</w>': 291,\n",
       " '8</w>': 292,\n",
       " '[</w>': 293,\n",
       " ']</w>': 294,\n",
       " '<</w>': 295,\n",
       " '&</w>': 296,\n",
       " '%</w>': 297,\n",
       " '¨</w>': 298,\n",
       " '`</w>': 299,\n",
       " 'é</w>': 300,\n",
       " '»</w>': 301,\n",
       " '«</w>': 302,\n",
       " '=</w>': 303,\n",
       " '•</w>': 304,\n",
       " '@</w>': 305,\n",
       " '+</w>': 306,\n",
       " '©</w>': 307,\n",
       " '¡</w>': 308,\n",
       " '{</w>': 309,\n",
       " '}</w>': 310,\n",
       " 'ª</w>': 311,\n",
       " 'ñ</w>': 312,\n",
       " 'ï</w>': 313,\n",
       " '‖</w>': 314,\n",
       " 'ç</w>': 315,\n",
       " 'í</w>': 316,\n",
       " '^</w>': 317,\n",
       " '£</w>': 318,\n",
       " '§</w>': 319,\n",
       " '♥</w>': 320,\n",
       " '−</w>': 321,\n",
       " 'à</w>': 322,\n",
       " '|</w>': 323,\n",
       " '°</w>': 324,\n",
       " '¦</w>': 325,\n",
       " 'ł</w>': 326,\n",
       " 'ĩ</w>': 327,\n",
       " 'ü</w>': 328,\n",
       " '®</w>': 329,\n",
       " 'ù</w>': 330,\n",
       " 'á</w>': 331,\n",
       " 'â</w>': 332,\n",
       " 'ó</w>': 333,\n",
       " 'è</w>': 334,\n",
       " '∞</w>': 335,\n",
       " 'ë</w>': 336,\n",
       " 'ä</w>': 337,\n",
       " '♪</w>': 338,\n",
       " 'ò</w>': 339,\n",
       " 'ω</w>': 340,\n",
       " '▪</w>': 341,\n",
       " '½</w>': 342,\n",
       " 'ǒ</w>': 343,\n",
       " '‡</w>': 344,\n",
       " 'ê</w>': 345,\n",
       " '◊</w>': 346,\n",
       " '►</w>': 347,\n",
       " '۞</w>': 348,\n",
       " 'ú</w>': 349,\n",
       " '€</w>': 350,\n",
       " 'æ</w>': 351,\n",
       " 'î</w>': 352,\n",
       " '↕</w>': 353,\n",
       " 'ô</w>': 354,\n",
       " 'ē</w>': 355,\n",
       " 'ǐ</w>': 356,\n",
       " '♫</w>': 357,\n",
       " '�</w>': 358,\n",
       " '\\uf04a</w>': 359,\n",
       " '™</w>': 360,\n",
       " 'ř</w>': 361,\n",
       " 'ā</w>': 362,\n",
       " '·</w>': 363,\n",
       " '¿</w>': 364,\n",
       " '\\\\</w>': 365,\n",
       " '─</w>': 366,\n",
       " '\\uf067</w>': 367,\n",
       " '\\uf020</w>': 368,\n",
       " '∙</w>': 369,\n",
       " 'ॐ</w>': 370,\n",
       " 'ö</w>': 371,\n",
       " 'ø</w>': 372,\n",
       " '\\uf06c</w>': 373,\n",
       " '\\uf09b</w>': 374,\n",
       " '●</w>': 375,\n",
       " '\\xad</w>': 376,\n",
       " '■</w>': 377,\n",
       " '\\uf063</w>': 378,\n",
       " '†</w>': 379,\n",
       " 'å</w>': 380,\n",
       " 'ō</w>': 381,\n",
       " 'ã</w>': 382,\n",
       " '¤</w>': 383,\n",
       " '⠔</w>': 384,\n",
       " '\\uf059</w>': 385,\n",
       " 'н</w>': 386,\n",
       " '\\uf09a</w>': 387,\n",
       " '⚔</w>': 388,\n",
       " 'ă</w>': 389,\n",
       " 'û</w>': 390,\n",
       " 'º</w>': 391,\n",
       " '♦</w>': 392,\n",
       " 'ĝ</w>': 393,\n",
       " '¹</w>': 394,\n",
       " '═</w>': 395,\n",
       " '\\uf0a3</w>': 396,\n",
       " '¾</w>': 397,\n",
       " 'ì</w>': 398,\n",
       " '☼</w>': 399,\n",
       " 'ș</w>': 400,\n",
       " '¼</w>': 401,\n",
       " '☺</w>': 402,\n",
       " 'đ</w>': 403,\n",
       " 'ą</w>': 404,\n",
       " 'ǽ</w>': 405,\n",
       " '╦</w>': 406,\n",
       " '\\uf02a</w>': 407,\n",
       " '¬</w>': 408,\n",
       " 'ī</w>': 409,\n",
       " '\\u200b</w>': 410,\n",
       " 'œ</w>': 411,\n",
       " '¢</w>': 412,\n",
       " 'ǎ</w>': 413,\n",
       " 'š</w>': 414,\n",
       " 'ʻ</w>': 415,\n",
       " 'ν</w>': 416,\n",
       " 'α</w>': 417,\n",
       " '\\uf05d</w>': 418,\n",
       " 'я</w>': 419,\n",
       " 'б</w>': 420,\n",
       " 'й</w>': 421,\n",
       " 'τ</w>': 422,\n",
       " 'ο</w>': 423,\n",
       " 'ε</w>': 424,\n",
       " 'ί</w>': 425,\n",
       " 'ι</w>': 426,\n",
       " 'δ</w>': 427,\n",
       " '\\uf073</w>': 428,\n",
       " '\\uf05e</w>': 429,\n",
       " '‐</w>': 430,\n",
       " 'с</w>': 431,\n",
       " '\\uf0a5</w>': 432,\n",
       " 'þ</w>': 433,\n",
       " 'κ</w>': 434,\n",
       " '‑</w>': 435,\n",
       " '‒</w>': 436,\n",
       " 'к</w>': 437,\n",
       " '\\uf07e</w>': 438,\n",
       " 'σ</w>': 439,\n",
       " 'č</w>': 440,\n",
       " '\\uf0be</w>': 441,\n",
       " 'υ</w>': 442,\n",
       " 'ό</w>': 443,\n",
       " '\\uf061</w>': 444,\n",
       " '\\uf02d</w>': 445,\n",
       " 'ß</w>': 446,\n",
       " 'е</w>': 447,\n",
       " 'т</w>': 448,\n",
       " 'μ</w>': 449,\n",
       " 'π</w>': 450,\n",
       " 'ρ</w>': 451,\n",
       " 'έ</w>': 452,\n",
       " 'в</w>': 453,\n",
       " 'ş</w>': 454,\n",
       " 'о</w>': 455,\n",
       " 'ð</w>': 456,\n",
       " 'а</w>': 457,\n",
       " 'ς</w>': 458,\n",
       " 'р</w>': 459,\n",
       " 'м</w>': 460,\n",
       " 'у</w>': 461,\n",
       " 'ή</w>': 462,\n",
       " 'ά</w>': 463,\n",
       " 'и</w>': 464,\n",
       " 'д</w>': 465,\n",
       " '\\uf0bc</w>': 466,\n",
       " '\\uf070</w>': 467,\n",
       " 'λ</w>': 468,\n",
       " 'л</w>': 469,\n",
       " 'γ</w>': 470,\n",
       " '¯</w>': 471,\n",
       " '\\uf065</w>': 472,\n",
       " '\\uf043</w>': 473,\n",
       " '\\uf074</w>': 474,\n",
       " '\\uf068</w>': 475,\n",
       " '\\uf072</w>': 476,\n",
       " 'th': 477,\n",
       " 'in': 478,\n",
       " 'ed</w>': 479,\n",
       " 'an': 480,\n",
       " 'the</w>': 481,\n",
       " 'ou': 482,\n",
       " 'er</w>': 483,\n",
       " 'ing</w>': 484,\n",
       " 'to</w>': 485,\n",
       " 'er': 486,\n",
       " 'he</w>': 487,\n",
       " 'and</w>': 488,\n",
       " 'ar': 489,\n",
       " 'hi': 490,\n",
       " 'at</w>': 491,\n",
       " 're': 492,\n",
       " 'wa': 493,\n",
       " 'on': 494,\n",
       " 'st': 495,\n",
       " 'en': 496,\n",
       " 'ha': 497,\n",
       " 'of</w>': 498,\n",
       " 'or': 499,\n",
       " 'in</w>': 500,\n",
       " 'al': 501,\n",
       " 'it': 502,\n",
       " 'en</w>': 503,\n",
       " 'on</w>': 504,\n",
       " 'el': 505,\n",
       " 'ro': 506,\n",
       " 'it</w>': 507,\n",
       " 'ac': 508,\n",
       " 'was</w>': 509,\n",
       " 'me</w>': 510,\n",
       " 'yo': 511,\n",
       " 'you</w>': 512,\n",
       " 'her</w>': 513,\n",
       " 'es</w>': 514,\n",
       " 'ly</w>': 515,\n",
       " 'no': 516,\n",
       " 'at': 517,\n",
       " 'lo': 518,\n",
       " 'li': 519,\n",
       " 'she</w>': 520,\n",
       " 'wh': 521,\n",
       " 'or</w>': 522,\n",
       " 'st</w>': 523,\n",
       " 'his</w>': 524,\n",
       " 'that</w>': 525,\n",
       " 'ea': 526,\n",
       " 've</w>': 527,\n",
       " 'be': 528,\n",
       " 'ri': 529,\n",
       " 'ld</w>': 530,\n",
       " 'an</w>': 531,\n",
       " 'gh': 532,\n",
       " 'ere</w>': 533,\n",
       " 'the': 534,\n",
       " \"'s</w>\": 535,\n",
       " 'ti': 536,\n",
       " \"'t</w>\": 537,\n",
       " \"n't</w>\": 538,\n",
       " 'id</w>': 539,\n",
       " 'sa': 540,\n",
       " 'le</w>': 541,\n",
       " 'si': 542,\n",
       " 'ur': 543,\n",
       " 'is</w>': 544,\n",
       " 'bu': 545,\n",
       " 'se</w>': 546,\n",
       " 'my</w>': 547,\n",
       " 'ho': 548,\n",
       " 'ould</w>': 549,\n",
       " 'ne': 550,\n",
       " 'out</w>': 551,\n",
       " 'le': 552,\n",
       " 'wit': 553,\n",
       " 'om': 554,\n",
       " 'il': 555,\n",
       " 'with</w>': 556,\n",
       " 'as</w>': 557,\n",
       " 'had</w>': 558,\n",
       " 'se': 559,\n",
       " 'ght</w>': 560,\n",
       " 'ke</w>': 561,\n",
       " 'for</w>': 562,\n",
       " 'un': 563,\n",
       " 'la': 564,\n",
       " 'ra': 565,\n",
       " 'one</w>': 566,\n",
       " 'ma': 567,\n",
       " 'but</w>': 568,\n",
       " 'do': 569,\n",
       " 'ab': 570,\n",
       " 'to': 571,\n",
       " 'ic': 572,\n",
       " 'ch': 573,\n",
       " 'ev': 574,\n",
       " 'him</w>': 575,\n",
       " 'sh': 576,\n",
       " 'ked</w>': 577,\n",
       " 'ca': 578,\n",
       " 'pp': 579,\n",
       " 'be</w>': 580,\n",
       " 'go': 581,\n",
       " 'sp': 582,\n",
       " 'oun': 583,\n",
       " 'ir': 584,\n",
       " 'de': 585,\n",
       " 'ther</w>': 586,\n",
       " 'do</w>': 587,\n",
       " 'co': 588,\n",
       " 'all</w>': 589,\n",
       " 'et</w>': 590,\n",
       " 'ss</w>': 591,\n",
       " 'di': 592,\n",
       " 'mo': 593,\n",
       " 'ent</w>': 594,\n",
       " 'not</w>': 595,\n",
       " 'de</w>': 596,\n",
       " 'now</w>': 597,\n",
       " 'ted</w>': 598,\n",
       " 'what</w>': 599,\n",
       " 'they</w>': 600,\n",
       " 'ag': 601,\n",
       " 'ack</w>': 602,\n",
       " 'said</w>': 603,\n",
       " 'have</w>': 604,\n",
       " 'fro': 605,\n",
       " 'we</w>': 606,\n",
       " 'ch</w>': 607,\n",
       " 'ce</w>': 608,\n",
       " 'up</w>': 609,\n",
       " 'ore</w>': 610,\n",
       " 'bo': 611,\n",
       " 'ver</w>': 612,\n",
       " 'ter</w>': 613,\n",
       " 'loo': 614,\n",
       " 'thing</w>': 615,\n",
       " 'this</w>': 616,\n",
       " 'from</w>': 617,\n",
       " 'king</w>': 618,\n",
       " 'ds</w>': 619,\n",
       " 'so</w>': 620,\n",
       " 'as': 621,\n",
       " 'our</w>': 622,\n",
       " 'su': 623,\n",
       " 'wn</w>': 624,\n",
       " 'con': 625,\n",
       " 'did</w>': 626,\n",
       " 'mi': 627,\n",
       " 'ru': 628,\n",
       " 'fe': 629,\n",
       " 'sed</w>': 630,\n",
       " 'gh</w>': 631,\n",
       " 'ta': 632,\n",
       " 'ju': 633,\n",
       " 'led</w>': 634,\n",
       " 'could</w>': 635,\n",
       " 'would</w>': 636,\n",
       " 'so': 637,\n",
       " 'way</w>': 638,\n",
       " 'ts</w>': 639,\n",
       " 'are</w>': 640,\n",
       " 'were</w>': 641,\n",
       " 'ir</w>': 642,\n",
       " 'da': 643,\n",
       " 'po': 644,\n",
       " 'if</w>': 645,\n",
       " 'em': 646,\n",
       " 'ill</w>': 647,\n",
       " 'rea': 648,\n",
       " 'like</w>': 649,\n",
       " 'ers</w>': 650,\n",
       " 'back</w>': 651,\n",
       " 'wor': 652,\n",
       " 'ear': 653,\n",
       " 'ound</w>': 654,\n",
       " 'there</w>': 655,\n",
       " \"'d</w>\": 656,\n",
       " 'ded</w>': 657,\n",
       " 'ell</w>': 658,\n",
       " 'ex': 659,\n",
       " 'qu': 660,\n",
       " 'ough</w>': 661,\n",
       " 'hea': 662,\n",
       " 'th</w>': 663,\n",
       " 'no</w>': 664,\n",
       " 'll</w>': 665,\n",
       " 'into</w>': 666,\n",
       " 'ing': 667,\n",
       " 'just</w>': 668,\n",
       " 'when</w>': 669,\n",
       " 'about</w>': 670,\n",
       " 'ati': 671,\n",
       " 'fa': 672,\n",
       " 'pu': 673,\n",
       " 'then</w>': 674,\n",
       " 'ally</w>': 675,\n",
       " 'sc': 676,\n",
       " 'lea': 677,\n",
       " 'ver': 678,\n",
       " 'al</w>': 679,\n",
       " 'mu': 680,\n",
       " 'ant</w>': 681,\n",
       " 'ace</w>': 682,\n",
       " 'fu': 683,\n",
       " 'whi': 684,\n",
       " 'yes</w>': 685,\n",
       " 'ind</w>': 686,\n",
       " 'ting</w>': 687,\n",
       " 'them</w>': 688,\n",
       " 'dy</w>': 689,\n",
       " 'com': 690,\n",
       " 'ding</w>': 691,\n",
       " 'gu': 692,\n",
       " 'tur': 693,\n",
       " 'been</w>': 694,\n",
       " 'ee': 695,\n",
       " 'for': 696,\n",
       " 'som': 697,\n",
       " 'ard</w>': 698,\n",
       " 'know</w>': 699,\n",
       " 'some': 700,\n",
       " 'op': 701,\n",
       " 'by</w>': 702,\n",
       " 'tw': 703,\n",
       " 'your</w>': 704,\n",
       " 'ter': 705,\n",
       " 'pro': 706,\n",
       " 'sel': 707,\n",
       " 'of': 708,\n",
       " 'ge</w>': 709,\n",
       " 'fi': 710,\n",
       " 'od</w>': 711,\n",
       " 'pa': 712,\n",
       " 'ec': 713,\n",
       " 'down</w>': 714,\n",
       " 'over</w>': 715,\n",
       " 're</w>': 716,\n",
       " 'lu': 717,\n",
       " 'how</w>': 718,\n",
       " \"'m</w>\": 719,\n",
       " 'time</w>': 720,\n",
       " 'aga': 721,\n",
       " 'wi': 722,\n",
       " 'tr': 723,\n",
       " 'sur': 724,\n",
       " 'more</w>': 725,\n",
       " '..': 726,\n",
       " 'get</w>': 727,\n",
       " 'other</w>': 728,\n",
       " 'pre': 729,\n",
       " 'ned</w>': 730,\n",
       " 'ong</w>': 731,\n",
       " 'der</w>': 732,\n",
       " 'vi': 733,\n",
       " 'par': 734,\n",
       " 'ys</w>': 735,\n",
       " 'pl': 736,\n",
       " 'side</w>': 737,\n",
       " 'fo': 738,\n",
       " 'tly</w>': 739,\n",
       " 'ck</w>': 740,\n",
       " 'eyes</w>': 741,\n",
       " 'ks</w>': 742,\n",
       " 'gi': 743,\n",
       " 'me': 744,\n",
       " 'ine</w>': 745,\n",
       " 'ate</w>': 746,\n",
       " 'ni': 747,\n",
       " 'self</w>': 748,\n",
       " '...</w>': 749,\n",
       " 'per': 750,\n",
       " 'ty</w>': 751,\n",
       " 'af': 752,\n",
       " 'el</w>': 753,\n",
       " 'their</w>': 754,\n",
       " 'ice</w>': 755,\n",
       " 'head</w>': 756,\n",
       " 'thin': 757,\n",
       " 'pped</w>': 758,\n",
       " 'can</w>': 759,\n",
       " 'gr': 760,\n",
       " \"'re</w>\": 761,\n",
       " 'man</w>': 762,\n",
       " 'who</w>': 763,\n",
       " 'ying</w>': 764,\n",
       " 'ling</w>': 765,\n",
       " 'ation</w>': 766,\n",
       " 'sto': 767,\n",
       " 'us</w>': 768,\n",
       " 'sm': 769,\n",
       " 'right</w>': 770,\n",
       " 'der': 771,\n",
       " 'sho': 772,\n",
       " 'ok</w>': 773,\n",
       " 'ge': 774,\n",
       " 'any</w>': 775,\n",
       " 'ga': 776,\n",
       " 'fore</w>': 777,\n",
       " 'pe': 778,\n",
       " 'ever': 779,\n",
       " 'ought</w>': 780,\n",
       " 'before</w>': 781,\n",
       " 'han': 782,\n",
       " 'new</w>': 783,\n",
       " 'even</w>': 784,\n",
       " 'around</w>': 785,\n",
       " 'ely</w>': 786,\n",
       " 'mp': 787,\n",
       " 'see</w>': 788,\n",
       " 'star': 789,\n",
       " 'cau': 790,\n",
       " 'any': 791,\n",
       " 'ved</w>': 792,\n",
       " 'here</w>': 793,\n",
       " 'ss': 794,\n",
       " 'sh</w>': 795,\n",
       " 'clo': 796,\n",
       " 'going</w>': 797,\n",
       " 'fir': 798,\n",
       " 'go</w>': 799,\n",
       " 'our': 800,\n",
       " 'thr': 801,\n",
       " 'ps</w>': 802,\n",
       " 'some</w>': 803,\n",
       " \"'ll</w>\": 804,\n",
       " 'low': 805,\n",
       " 'where</w>': 806,\n",
       " 'ving</w>': 807,\n",
       " 'only</w>': 808,\n",
       " 'tion</w>': 809,\n",
       " 'hel': 810,\n",
       " 'off</w>': 811,\n",
       " 'will</w>': 812,\n",
       " 'na': 813,\n",
       " 'ci': 814,\n",
       " 'than</w>': 815,\n",
       " 'looked</w>': 816,\n",
       " 'able</w>': 817,\n",
       " 'tle</w>': 818,\n",
       " 'roo': 819,\n",
       " 'ons</w>': 820,\n",
       " 'ten': 821,\n",
       " 'through</w>': 822,\n",
       " 'want</w>': 823,\n",
       " 'ous</w>': 824,\n",
       " 'think</w>': 825,\n",
       " 'ning</w>': 826,\n",
       " 'cu': 827,\n",
       " 'hand</w>': 828,\n",
       " 'ba': 829,\n",
       " 'vo': 830,\n",
       " 'mar': 831,\n",
       " 'jo': 832,\n",
       " 'again</w>': 833,\n",
       " 'too</w>': 834,\n",
       " 'face</w>': 835,\n",
       " 'te</w>': 836,\n",
       " 'wal': 837,\n",
       " 'shi': 838,\n",
       " 'sw': 839,\n",
       " 'lit': 840,\n",
       " 'away</w>': 841,\n",
       " 'ft</w>': 842,\n",
       " 'still</w>': 843,\n",
       " 'room</w>': 844,\n",
       " 'ity</w>': 845,\n",
       " 'something</w>': 846,\n",
       " 'fe</w>': 847,\n",
       " 'come</w>': 848,\n",
       " 'ssi': 849,\n",
       " 'day</w>': 850,\n",
       " 'let</w>': 851,\n",
       " 'ry</w>': 852,\n",
       " 'ear</w>': 853,\n",
       " 'ep': 854,\n",
       " 'ings</w>': 855,\n",
       " 'gre': 856,\n",
       " 'car': 857,\n",
       " 'ered</w>': 858,\n",
       " 'est': 859,\n",
       " 'wan': 860,\n",
       " 'after</w>': 861,\n",
       " 'well</w>': 862,\n",
       " 'hear': 863,\n",
       " 'asked</w>': 864,\n",
       " 'bl': 865,\n",
       " 'thought</w>': 866,\n",
       " 'two</w>': 867,\n",
       " 'never</w>': 868,\n",
       " 'ang': 869,\n",
       " 'good</w>': 870,\n",
       " 'ever</w>': 871,\n",
       " 'end</w>': 872,\n",
       " 'sta': 873,\n",
       " 'ad': 874,\n",
       " 'ated</w>': 875,\n",
       " 'br': 876,\n",
       " 'ance</w>': 877,\n",
       " 'min': 878,\n",
       " 'cha': 879,\n",
       " \"'ve</w>\": 880,\n",
       " 'sure</w>': 881,\n",
       " 'ck': 882,\n",
       " 'cause</w>': 883,\n",
       " 'hu': 884,\n",
       " 'made</w>': 885,\n",
       " 'got</w>': 886,\n",
       " 'tri': 887,\n",
       " 'ssed</w>': 888,\n",
       " 'much</w>': 889,\n",
       " 'look</w>': 890,\n",
       " 'ched</w>': 891,\n",
       " 'mb': 892,\n",
       " 'shed</w>': 893,\n",
       " 'fin': 894,\n",
       " 'why</w>': 895,\n",
       " 'du': 896,\n",
       " 'ward</w>': 897,\n",
       " 'bel': 898,\n",
       " 'turned</w>': 899,\n",
       " 'sha': 900,\n",
       " 'gg': 901,\n",
       " 'ach</w>': 902,\n",
       " 'bro': 903,\n",
       " 'gra': 904,\n",
       " 'most</w>': 905,\n",
       " 'knew</w>': 906,\n",
       " 'ath</w>': 907,\n",
       " 'door</w>': 908,\n",
       " 'little</w>': 909,\n",
       " 'tal': 910,\n",
       " 'ls</w>': 911,\n",
       " 'because</w>': 912,\n",
       " 'fel': 913,\n",
       " 'ened</w>': 914,\n",
       " 'tu': 915,\n",
       " 'war': 916,\n",
       " 'te': 917,\n",
       " 'sk': 918,\n",
       " 'ff': 919,\n",
       " 'sit': 920,\n",
       " 'take</w>': 921,\n",
       " 'happ': 922,\n",
       " 'man': 923,\n",
       " 'ms</w>': 924,\n",
       " 'make</w>': 925,\n",
       " 'cal': 926,\n",
       " 'every': 927,\n",
       " 'long</w>': 928,\n",
       " 'first</w>': 929,\n",
       " 'tra': 930,\n",
       " 'ach': 931,\n",
       " 'ste': 932,\n",
       " 'ful</w>': 933,\n",
       " 'ble</w>': 934,\n",
       " 'ess</w>': 935,\n",
       " 'im': 936,\n",
       " 'say</w>': 937,\n",
       " 'ence</w>': 938,\n",
       " 'came</w>': 939,\n",
       " 'ced</w>': 940,\n",
       " 'pri': 941,\n",
       " 'felt</w>': 942,\n",
       " 'bed</w>': 943,\n",
       " 'ree</w>': 944,\n",
       " 'son</w>': 945,\n",
       " 'mon': 946,\n",
       " 'dar': 947,\n",
       " 'took</w>': 948,\n",
       " 'ser': 949,\n",
       " 'app': 950,\n",
       " 'ki': 951,\n",
       " 'tru': 952,\n",
       " 'fri': 953,\n",
       " 'low</w>': 954,\n",
       " 'chi': 955,\n",
       " 'body</w>': 956,\n",
       " 'fr': 957,\n",
       " 'pla': 958,\n",
       " 'sin': 959,\n",
       " 'ali': 960,\n",
       " 'wanted</w>': 961,\n",
       " 'ose</w>': 962,\n",
       " 'very</w>': 963,\n",
       " 'ves</w>': 964,\n",
       " 'est</w>': 965,\n",
       " 'need</w>': 966,\n",
       " 'pul': 967,\n",
       " 'kno': 968,\n",
       " 'ears</w>': 969,\n",
       " 'dd': 970,\n",
       " 'stu': 971,\n",
       " 'tell</w>': 972,\n",
       " 'pi': 973,\n",
       " 'str': 974,\n",
       " 'dre': 975,\n",
       " 'really</w>': 976,\n",
       " 'cre': 977,\n",
       " 'red</w>': 978,\n",
       " 'bi': 979,\n",
       " 'has</w>': 980,\n",
       " 'cont': 981,\n",
       " 'he': 982,\n",
       " 'hands</w>': 983,\n",
       " 'which</w>': 984,\n",
       " 'sen': 985,\n",
       " 'peop': 986,\n",
       " 'its</w>': 987,\n",
       " 'sing</w>': 988,\n",
       " 'people</w>': 989,\n",
       " 'rec': 990,\n",
       " 'wat': 991,\n",
       " 'sli': 992,\n",
       " 'ca</w>': 993,\n",
       " 'should</w>': 994,\n",
       " 'night</w>': 995,\n",
       " 'ws</w>': 996,\n",
       " 'with': 997,\n",
       " 'though</w>': 998,\n",
       " 'left</w>': 999,\n",
       " 'while</w>': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "bpe_tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "print(len(bpe_tokenizer.get_vocab()))\n",
    "bpe_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['special</w>',\n",
       " 'cases</w>',\n",
       " 'are</w>',\n",
       " \"n't</w>\",\n",
       " 'special</w>',\n",
       " 'enough</w>',\n",
       " 'to</w>',\n",
       " 'break</w>',\n",
       " 'the</w>',\n",
       " 'rules</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.tokenize(python_zen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m.</w>', 'night</w>', 'shy', 'am', 'alan</w>']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like directors\n",
    "bpe_tokenizer.tokenize('M. Night Shyamalan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':</w>', 'hippo', 'po', 'tom', 'on', 'stro', 'se', 'squi', 'pp', 'ed', 'ali', 'op', 'ho', 'bia</w>']\n"
     ]
    }
   ],
   "source": [
    "# The fear of long words\n",
    "print(bpe_tokenizer.tokenize(\":Hippopotomonstrosesquippedaliophobia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['projec', 't', 'contrac', 't', 'char', 'ging', 'peri', 'od', 'projec', 't', 'accoun', 'tre', 'fer', 'en', 'c', 'ev', 'm</w>']\n"
     ]
    }
   ],
   "source": [
    "# A very long Java Class name\n",
    "print(bpe_tokenizer.tokenize('ProjectContractChargingPeriodProjectAccountReferenceVM'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2983, 5767, 640, 538, 2983, 1046, 485, 2158, 481, 4556, 239], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer(python_zen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Looks a lot like what we had at the start ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordPiece\n",
    "- Used by: BERT, distilBERT, Electra\n",
    "- Similar to BPE: starts with a base vocabulary that includes every character, but merges based on maximum likelihood of the training data, rather than just frequency. \n",
    "- Is a Google internal model without Open Source implementation. SentencePiece is Google's open source version of this. We don't really know how much they differ. \n",
    "- First proposed by [Schuster and Nakajima, 2015](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), in the paper called *Japanese and Korean Voice Search*. Both working at Google. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparisons with other implementations\n",
    "\n",
    "|Feature|SentencePiece|[subword-nmt](https://github.com/rsennrich/subword-nmt)|[WordPiece](https://arxiv.org/pdf/1609.08144.pdf)|\n",
    "|:---|:---:|:---:|:---:|\n",
    "|Supported algorithm|BPE, unigram, char, word|BPE|BPE*|\n",
    "|OSS?|Yes|Yes|Google internal|\n",
    "|Subword regularization|[Yes](#subword-regularization)|No|No|\n",
    "|Python Library (pip)|[Yes](python/README.md)|No|N/A|\n",
    "|C++ Library|[Yes](doc/api.md)|No|N/A|\n",
    "|Pre-segmentation required?|[No](#whitespace-is-treated-as-a-basic-symbol)|Yes|Yes|\n",
    "|Customizable normalization (e.g., NFKC)|[Yes](doc/normalization.md)|No|N/A|\n",
    "|Direct id generation|[Yes](#end-to-end-example)|No|N/A|\n",
    "\n",
    "*From the [SentencePiece Github](https://github.com/google/sentencepiece).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Differences with BPE\n",
    "- Subwords are identified with a prefix (`##`)\n",
    "- If no subword can be found in the vocabulary, the whole word is tokenized as `[UNK]`. So the previous example of encoding `mug` as `[\"<UNK>\", \"ug\"]` cannot happen with wordpiece. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Merging rules\n",
    "Search for the symbol pair with the highest score. This prioritizes combining pairs that aren't as frequent in the vocabulary. \n",
    "$$ score_{ij} = \\frac{f_{ij}}{f_i \\cdot f_j} $$\n",
    "\n",
    "If $f_i \\cdot f_j$ is very small, the score becomes very large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So say we have the same words as before:\n",
    "```yaml\n",
    "Corpus: (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "\n",
    "Tokenizing with the start vocabulary:\n",
    "```yaml\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]\n",
    "Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. The most frequent pair is (`##u`, `##g`), 20 times, but the individual frequency of `##u` is very high. \n",
    "2. The result is that the pair's score is only $\\frac{20}{36\\times20} = \\frac{1}{36}$. \n",
    "3. This goes for all combinations with `##u`. \n",
    "4. The highest scoring pair is (`##g`, `##s`) - at $\\frac{5}{20\\times5}=\\frac{1}{20}$. This is also the only pair without `##u` in it. \n",
    "\n",
    "First merge is (`##g`, `##s`) --> (`##gs`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then get, with our new token:\n",
    "```yaml\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"]\n",
    "Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5)\n",
    "```\n",
    "\n",
    "And the next step, where `hu` is the token with the highest likelihood increase to the corpus. \n",
    "\n",
    "```yaml\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"]\n",
    "Corpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, we apply this iteratively until we have our desired vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[unused1]': 1,\n",
       " '[unused2]': 2,\n",
       " '[unused3]': 3,\n",
       " '[unused4]': 4,\n",
       " '[unused5]': 5,\n",
       " '[unused6]': 6,\n",
       " '[unused7]': 7,\n",
       " '[unused8]': 8,\n",
       " '[unused9]': 9,\n",
       " '[unused10]': 10,\n",
       " '[unused11]': 11,\n",
       " '[unused12]': 12,\n",
       " '[unused13]': 13,\n",
       " '[unused14]': 14,\n",
       " '[unused15]': 15,\n",
       " '[unused16]': 16,\n",
       " '[unused17]': 17,\n",
       " '[unused18]': 18,\n",
       " '[unused19]': 19,\n",
       " '[unused20]': 20,\n",
       " '[unused21]': 21,\n",
       " '[unused22]': 22,\n",
       " '[unused23]': 23,\n",
       " '[unused24]': 24,\n",
       " '[unused25]': 25,\n",
       " '[unused26]': 26,\n",
       " '[unused27]': 27,\n",
       " '[unused28]': 28,\n",
       " '[unused29]': 29,\n",
       " '[unused30]': 30,\n",
       " '[unused31]': 31,\n",
       " '[unused32]': 32,\n",
       " '[unused33]': 33,\n",
       " '[unused34]': 34,\n",
       " '[unused35]': 35,\n",
       " '[unused36]': 36,\n",
       " '[unused37]': 37,\n",
       " '[unused38]': 38,\n",
       " '[unused39]': 39,\n",
       " '[unused40]': 40,\n",
       " '[unused41]': 41,\n",
       " '[unused42]': 42,\n",
       " '[unused43]': 43,\n",
       " '[unused44]': 44,\n",
       " '[unused45]': 45,\n",
       " '[unused46]': 46,\n",
       " '[unused47]': 47,\n",
       " '[unused48]': 48,\n",
       " '[unused49]': 49,\n",
       " '[unused50]': 50,\n",
       " '[unused51]': 51,\n",
       " '[unused52]': 52,\n",
       " '[unused53]': 53,\n",
       " '[unused54]': 54,\n",
       " '[unused55]': 55,\n",
       " '[unused56]': 56,\n",
       " '[unused57]': 57,\n",
       " '[unused58]': 58,\n",
       " '[unused59]': 59,\n",
       " '[unused60]': 60,\n",
       " '[unused61]': 61,\n",
       " '[unused62]': 62,\n",
       " '[unused63]': 63,\n",
       " '[unused64]': 64,\n",
       " '[unused65]': 65,\n",
       " '[unused66]': 66,\n",
       " '[unused67]': 67,\n",
       " '[unused68]': 68,\n",
       " '[unused69]': 69,\n",
       " '[unused70]': 70,\n",
       " '[unused71]': 71,\n",
       " '[unused72]': 72,\n",
       " '[unused73]': 73,\n",
       " '[unused74]': 74,\n",
       " '[unused75]': 75,\n",
       " '[unused76]': 76,\n",
       " '[unused77]': 77,\n",
       " '[unused78]': 78,\n",
       " '[unused79]': 79,\n",
       " '[unused80]': 80,\n",
       " '[unused81]': 81,\n",
       " '[unused82]': 82,\n",
       " '[unused83]': 83,\n",
       " '[unused84]': 84,\n",
       " '[unused85]': 85,\n",
       " '[unused86]': 86,\n",
       " '[unused87]': 87,\n",
       " '[unused88]': 88,\n",
       " '[unused89]': 89,\n",
       " '[unused90]': 90,\n",
       " '[unused91]': 91,\n",
       " '[unused92]': 92,\n",
       " '[unused93]': 93,\n",
       " '[unused94]': 94,\n",
       " '[unused95]': 95,\n",
       " '[unused96]': 96,\n",
       " '[unused97]': 97,\n",
       " '[unused98]': 98,\n",
       " '[unused99]': 99,\n",
       " '[UNK]': 100,\n",
       " '[CLS]': 101,\n",
       " '[SEP]': 102,\n",
       " '[MASK]': 103,\n",
       " '[unused100]': 104,\n",
       " '[unused101]': 105,\n",
       " '!': 106,\n",
       " '\"': 107,\n",
       " '#': 108,\n",
       " '$': 109,\n",
       " '%': 110,\n",
       " '&': 111,\n",
       " \"'\": 112,\n",
       " '(': 113,\n",
       " ')': 114,\n",
       " '*': 115,\n",
       " '+': 116,\n",
       " ',': 117,\n",
       " '-': 118,\n",
       " '.': 119,\n",
       " '/': 120,\n",
       " '0': 121,\n",
       " '1': 122,\n",
       " '2': 123,\n",
       " '3': 124,\n",
       " '4': 125,\n",
       " '5': 126,\n",
       " '6': 127,\n",
       " '7': 128,\n",
       " '8': 129,\n",
       " '9': 130,\n",
       " ':': 131,\n",
       " ';': 132,\n",
       " '<': 133,\n",
       " '=': 134,\n",
       " '>': 135,\n",
       " '?': 136,\n",
       " '@': 137,\n",
       " 'A': 138,\n",
       " 'B': 139,\n",
       " 'C': 140,\n",
       " 'D': 141,\n",
       " 'E': 142,\n",
       " 'F': 143,\n",
       " 'G': 144,\n",
       " 'H': 145,\n",
       " 'I': 146,\n",
       " 'J': 147,\n",
       " 'K': 148,\n",
       " 'L': 149,\n",
       " 'M': 150,\n",
       " 'N': 151,\n",
       " 'O': 152,\n",
       " 'P': 153,\n",
       " 'Q': 154,\n",
       " 'R': 155,\n",
       " 'S': 156,\n",
       " 'T': 157,\n",
       " 'U': 158,\n",
       " 'V': 159,\n",
       " 'W': 160,\n",
       " 'X': 161,\n",
       " 'Y': 162,\n",
       " 'Z': 163,\n",
       " '[': 164,\n",
       " '\\\\': 165,\n",
       " ']': 166,\n",
       " '^': 167,\n",
       " '_': 168,\n",
       " '`': 169,\n",
       " 'a': 170,\n",
       " 'b': 171,\n",
       " 'c': 172,\n",
       " 'd': 173,\n",
       " 'e': 174,\n",
       " 'f': 175,\n",
       " 'g': 176,\n",
       " 'h': 177,\n",
       " 'i': 178,\n",
       " 'j': 179,\n",
       " 'k': 180,\n",
       " 'l': 181,\n",
       " 'm': 182,\n",
       " 'n': 183,\n",
       " 'o': 184,\n",
       " 'p': 185,\n",
       " 'q': 186,\n",
       " 'r': 187,\n",
       " 's': 188,\n",
       " 't': 189,\n",
       " 'u': 190,\n",
       " 'v': 191,\n",
       " 'w': 192,\n",
       " 'x': 193,\n",
       " 'y': 194,\n",
       " 'z': 195,\n",
       " '{': 196,\n",
       " '|': 197,\n",
       " '}': 198,\n",
       " '~': 199,\n",
       " '¡': 200,\n",
       " '¢': 201,\n",
       " '£': 202,\n",
       " '¥': 203,\n",
       " '§': 204,\n",
       " '¨': 205,\n",
       " '©': 206,\n",
       " 'ª': 207,\n",
       " '«': 208,\n",
       " '¬': 209,\n",
       " '®': 210,\n",
       " '°': 211,\n",
       " '±': 212,\n",
       " '²': 213,\n",
       " '³': 214,\n",
       " '´': 215,\n",
       " 'µ': 216,\n",
       " '¶': 217,\n",
       " '·': 218,\n",
       " '¹': 219,\n",
       " 'º': 220,\n",
       " '»': 221,\n",
       " '¼': 222,\n",
       " '½': 223,\n",
       " '¾': 224,\n",
       " '¿': 225,\n",
       " 'À': 226,\n",
       " 'Á': 227,\n",
       " 'Â': 228,\n",
       " 'Ä': 229,\n",
       " 'Å': 230,\n",
       " 'Æ': 231,\n",
       " 'Ç': 232,\n",
       " 'È': 233,\n",
       " 'É': 234,\n",
       " 'Í': 235,\n",
       " 'Î': 236,\n",
       " 'Ñ': 237,\n",
       " 'Ó': 238,\n",
       " 'Ö': 239,\n",
       " '×': 240,\n",
       " 'Ø': 241,\n",
       " 'Ú': 242,\n",
       " 'Ü': 243,\n",
       " 'Þ': 244,\n",
       " 'ß': 245,\n",
       " 'à': 246,\n",
       " 'á': 247,\n",
       " 'â': 248,\n",
       " 'ã': 249,\n",
       " 'ä': 250,\n",
       " 'å': 251,\n",
       " 'æ': 252,\n",
       " 'ç': 253,\n",
       " 'è': 254,\n",
       " 'é': 255,\n",
       " 'ê': 256,\n",
       " 'ë': 257,\n",
       " 'ì': 258,\n",
       " 'í': 259,\n",
       " 'î': 260,\n",
       " 'ï': 261,\n",
       " 'ð': 262,\n",
       " 'ñ': 263,\n",
       " 'ò': 264,\n",
       " 'ó': 265,\n",
       " 'ô': 266,\n",
       " 'õ': 267,\n",
       " 'ö': 268,\n",
       " '÷': 269,\n",
       " 'ø': 270,\n",
       " 'ù': 271,\n",
       " 'ú': 272,\n",
       " 'û': 273,\n",
       " 'ü': 274,\n",
       " 'ý': 275,\n",
       " 'þ': 276,\n",
       " 'ÿ': 277,\n",
       " 'Ā': 278,\n",
       " 'ā': 279,\n",
       " 'ă': 280,\n",
       " 'ą': 281,\n",
       " 'Ć': 282,\n",
       " 'ć': 283,\n",
       " 'Č': 284,\n",
       " 'č': 285,\n",
       " 'ď': 286,\n",
       " 'Đ': 287,\n",
       " 'đ': 288,\n",
       " 'ē': 289,\n",
       " 'ė': 290,\n",
       " 'ę': 291,\n",
       " 'ě': 292,\n",
       " 'ğ': 293,\n",
       " 'ġ': 294,\n",
       " 'Ħ': 295,\n",
       " 'ħ': 296,\n",
       " 'ĩ': 297,\n",
       " 'Ī': 298,\n",
       " 'ī': 299,\n",
       " 'İ': 300,\n",
       " 'ı': 301,\n",
       " 'ļ': 302,\n",
       " 'Ľ': 303,\n",
       " 'ľ': 304,\n",
       " 'Ł': 305,\n",
       " 'ł': 306,\n",
       " 'ń': 307,\n",
       " 'ņ': 308,\n",
       " 'ň': 309,\n",
       " 'ŋ': 310,\n",
       " 'Ō': 311,\n",
       " 'ō': 312,\n",
       " 'ŏ': 313,\n",
       " 'ő': 314,\n",
       " 'Œ': 315,\n",
       " 'œ': 316,\n",
       " 'ř': 317,\n",
       " 'Ś': 318,\n",
       " 'ś': 319,\n",
       " 'Ş': 320,\n",
       " 'ş': 321,\n",
       " 'Š': 322,\n",
       " 'š': 323,\n",
       " 'Ţ': 324,\n",
       " 'ţ': 325,\n",
       " 'ť': 326,\n",
       " 'ũ': 327,\n",
       " 'ū': 328,\n",
       " 'ŭ': 329,\n",
       " 'ů': 330,\n",
       " 'ű': 331,\n",
       " 'ų': 332,\n",
       " 'ŵ': 333,\n",
       " 'ŷ': 334,\n",
       " 'ź': 335,\n",
       " 'Ż': 336,\n",
       " 'ż': 337,\n",
       " 'Ž': 338,\n",
       " 'ž': 339,\n",
       " 'Ə': 340,\n",
       " 'ƒ': 341,\n",
       " 'ơ': 342,\n",
       " 'ư': 343,\n",
       " 'ǎ': 344,\n",
       " 'ǐ': 345,\n",
       " 'ǒ': 346,\n",
       " 'ǔ': 347,\n",
       " 'ǫ': 348,\n",
       " 'Ș': 349,\n",
       " 'ș': 350,\n",
       " 'Ț': 351,\n",
       " 'ț': 352,\n",
       " 'ɐ': 353,\n",
       " 'ɑ': 354,\n",
       " 'ɔ': 355,\n",
       " 'ɕ': 356,\n",
       " 'ə': 357,\n",
       " 'ɛ': 358,\n",
       " 'ɡ': 359,\n",
       " 'ɣ': 360,\n",
       " 'ɨ': 361,\n",
       " 'ɪ': 362,\n",
       " 'ɲ': 363,\n",
       " 'ɾ': 364,\n",
       " 'ʀ': 365,\n",
       " 'ʁ': 366,\n",
       " 'ʂ': 367,\n",
       " 'ʃ': 368,\n",
       " 'ʊ': 369,\n",
       " 'ʋ': 370,\n",
       " 'ʌ': 371,\n",
       " 'ʐ': 372,\n",
       " 'ʑ': 373,\n",
       " 'ʒ': 374,\n",
       " 'ʔ': 375,\n",
       " 'ʰ': 376,\n",
       " 'ʲ': 377,\n",
       " 'ʳ': 378,\n",
       " 'ʷ': 379,\n",
       " 'ʻ': 380,\n",
       " 'ʼ': 381,\n",
       " 'ʾ': 382,\n",
       " 'ʿ': 383,\n",
       " 'ˈ': 384,\n",
       " 'ː': 385,\n",
       " 'ˡ': 386,\n",
       " 'ˢ': 387,\n",
       " 'ˣ': 388,\n",
       " '́': 389,\n",
       " '̃': 390,\n",
       " '̍': 391,\n",
       " '̯': 392,\n",
       " '͡': 393,\n",
       " 'Α': 394,\n",
       " 'Β': 395,\n",
       " 'Γ': 396,\n",
       " 'Δ': 397,\n",
       " 'Ε': 398,\n",
       " 'Η': 399,\n",
       " 'Θ': 400,\n",
       " 'Ι': 401,\n",
       " 'Κ': 402,\n",
       " 'Λ': 403,\n",
       " 'Μ': 404,\n",
       " 'Ν': 405,\n",
       " 'Ο': 406,\n",
       " 'Π': 407,\n",
       " 'Σ': 408,\n",
       " 'Τ': 409,\n",
       " 'Φ': 410,\n",
       " 'Χ': 411,\n",
       " 'Ψ': 412,\n",
       " 'Ω': 413,\n",
       " 'ά': 414,\n",
       " 'έ': 415,\n",
       " 'ή': 416,\n",
       " 'ί': 417,\n",
       " 'α': 418,\n",
       " 'β': 419,\n",
       " 'γ': 420,\n",
       " 'δ': 421,\n",
       " 'ε': 422,\n",
       " 'ζ': 423,\n",
       " 'η': 424,\n",
       " 'θ': 425,\n",
       " 'ι': 426,\n",
       " 'κ': 427,\n",
       " 'λ': 428,\n",
       " 'μ': 429,\n",
       " 'ν': 430,\n",
       " 'ξ': 431,\n",
       " 'ο': 432,\n",
       " 'π': 433,\n",
       " 'ρ': 434,\n",
       " 'ς': 435,\n",
       " 'σ': 436,\n",
       " 'τ': 437,\n",
       " 'υ': 438,\n",
       " 'φ': 439,\n",
       " 'χ': 440,\n",
       " 'ψ': 441,\n",
       " 'ω': 442,\n",
       " 'ό': 443,\n",
       " 'ύ': 444,\n",
       " 'ώ': 445,\n",
       " 'І': 446,\n",
       " 'Ј': 447,\n",
       " 'А': 448,\n",
       " 'Б': 449,\n",
       " 'В': 450,\n",
       " 'Г': 451,\n",
       " 'Д': 452,\n",
       " 'Е': 453,\n",
       " 'Ж': 454,\n",
       " 'З': 455,\n",
       " 'И': 456,\n",
       " 'К': 457,\n",
       " 'Л': 458,\n",
       " 'М': 459,\n",
       " 'Н': 460,\n",
       " 'О': 461,\n",
       " 'П': 462,\n",
       " 'Р': 463,\n",
       " 'С': 464,\n",
       " 'Т': 465,\n",
       " 'У': 466,\n",
       " 'Ф': 467,\n",
       " 'Х': 468,\n",
       " 'Ц': 469,\n",
       " 'Ч': 470,\n",
       " 'Ш': 471,\n",
       " 'Э': 472,\n",
       " 'Ю': 473,\n",
       " 'Я': 474,\n",
       " 'а': 475,\n",
       " 'б': 476,\n",
       " 'в': 477,\n",
       " 'г': 478,\n",
       " 'д': 479,\n",
       " 'е': 480,\n",
       " 'ж': 481,\n",
       " 'з': 482,\n",
       " 'и': 483,\n",
       " 'й': 484,\n",
       " 'к': 485,\n",
       " 'л': 486,\n",
       " 'м': 487,\n",
       " 'н': 488,\n",
       " 'о': 489,\n",
       " 'п': 490,\n",
       " 'р': 491,\n",
       " 'с': 492,\n",
       " 'т': 493,\n",
       " 'у': 494,\n",
       " 'ф': 495,\n",
       " 'х': 496,\n",
       " 'ц': 497,\n",
       " 'ч': 498,\n",
       " 'ш': 499,\n",
       " 'щ': 500,\n",
       " 'ъ': 501,\n",
       " 'ы': 502,\n",
       " 'ь': 503,\n",
       " 'э': 504,\n",
       " 'ю': 505,\n",
       " 'я': 506,\n",
       " 'ё': 507,\n",
       " 'і': 508,\n",
       " 'ї': 509,\n",
       " 'ј': 510,\n",
       " 'њ': 511,\n",
       " 'ћ': 512,\n",
       " 'Ա': 513,\n",
       " 'Հ': 514,\n",
       " 'ա': 515,\n",
       " 'ե': 516,\n",
       " 'ի': 517,\n",
       " 'կ': 518,\n",
       " 'մ': 519,\n",
       " 'յ': 520,\n",
       " 'ն': 521,\n",
       " 'ո': 522,\n",
       " 'ս': 523,\n",
       " 'տ': 524,\n",
       " 'ր': 525,\n",
       " 'ւ': 526,\n",
       " 'ְ': 527,\n",
       " 'ִ': 528,\n",
       " 'ֵ': 529,\n",
       " 'ֶ': 530,\n",
       " 'ַ': 531,\n",
       " 'ָ': 532,\n",
       " 'ֹ': 533,\n",
       " 'ּ': 534,\n",
       " 'א': 535,\n",
       " 'ב': 536,\n",
       " 'ג': 537,\n",
       " 'ד': 538,\n",
       " 'ה': 539,\n",
       " 'ו': 540,\n",
       " 'ז': 541,\n",
       " 'ח': 542,\n",
       " 'ט': 543,\n",
       " 'י': 544,\n",
       " 'כ': 545,\n",
       " 'ל': 546,\n",
       " 'ם': 547,\n",
       " 'מ': 548,\n",
       " 'ן': 549,\n",
       " 'נ': 550,\n",
       " 'ס': 551,\n",
       " 'ע': 552,\n",
       " 'פ': 553,\n",
       " 'צ': 554,\n",
       " 'ק': 555,\n",
       " 'ר': 556,\n",
       " 'ש': 557,\n",
       " 'ת': 558,\n",
       " '،': 559,\n",
       " 'ء': 560,\n",
       " 'آ': 561,\n",
       " 'أ': 562,\n",
       " 'إ': 563,\n",
       " 'ئ': 564,\n",
       " 'ا': 565,\n",
       " 'ب': 566,\n",
       " 'ة': 567,\n",
       " 'ت': 568,\n",
       " 'ث': 569,\n",
       " 'ج': 570,\n",
       " 'ح': 571,\n",
       " 'خ': 572,\n",
       " 'د': 573,\n",
       " 'ذ': 574,\n",
       " 'ر': 575,\n",
       " 'ز': 576,\n",
       " 'س': 577,\n",
       " 'ش': 578,\n",
       " 'ص': 579,\n",
       " 'ض': 580,\n",
       " 'ط': 581,\n",
       " 'ظ': 582,\n",
       " 'ع': 583,\n",
       " 'غ': 584,\n",
       " 'ف': 585,\n",
       " 'ق': 586,\n",
       " 'ك': 587,\n",
       " 'ل': 588,\n",
       " 'م': 589,\n",
       " 'ن': 590,\n",
       " 'ه': 591,\n",
       " 'و': 592,\n",
       " 'ى': 593,\n",
       " 'ي': 594,\n",
       " 'َ': 595,\n",
       " 'ِ': 596,\n",
       " 'ٹ': 597,\n",
       " 'پ': 598,\n",
       " 'چ': 599,\n",
       " 'ک': 600,\n",
       " 'گ': 601,\n",
       " 'ہ': 602,\n",
       " 'ی': 603,\n",
       " 'ے': 604,\n",
       " 'ं': 605,\n",
       " 'आ': 606,\n",
       " 'क': 607,\n",
       " 'ग': 608,\n",
       " 'च': 609,\n",
       " 'ज': 610,\n",
       " 'ण': 611,\n",
       " 'त': 612,\n",
       " 'द': 613,\n",
       " 'ध': 614,\n",
       " 'न': 615,\n",
       " 'प': 616,\n",
       " 'ब': 617,\n",
       " 'भ': 618,\n",
       " 'म': 619,\n",
       " 'य': 620,\n",
       " 'र': 621,\n",
       " 'ल': 622,\n",
       " 'व': 623,\n",
       " 'श': 624,\n",
       " 'ष': 625,\n",
       " 'स': 626,\n",
       " 'ह': 627,\n",
       " 'ा': 628,\n",
       " 'ि': 629,\n",
       " 'ी': 630,\n",
       " 'ु': 631,\n",
       " 'े': 632,\n",
       " 'ो': 633,\n",
       " '्': 634,\n",
       " '।': 635,\n",
       " '॥': 636,\n",
       " 'আ': 637,\n",
       " 'ই': 638,\n",
       " 'এ': 639,\n",
       " 'ও': 640,\n",
       " 'ক': 641,\n",
       " 'খ': 642,\n",
       " 'গ': 643,\n",
       " 'চ': 644,\n",
       " 'ছ': 645,\n",
       " 'জ': 646,\n",
       " 'ট': 647,\n",
       " 'ত': 648,\n",
       " 'থ': 649,\n",
       " 'দ': 650,\n",
       " 'ধ': 651,\n",
       " 'ন': 652,\n",
       " 'প': 653,\n",
       " 'ব': 654,\n",
       " 'ম': 655,\n",
       " 'য': 656,\n",
       " 'র': 657,\n",
       " 'ল': 658,\n",
       " 'শ': 659,\n",
       " 'স': 660,\n",
       " 'হ': 661,\n",
       " '়': 662,\n",
       " 'া': 663,\n",
       " 'ি': 664,\n",
       " 'ী': 665,\n",
       " 'ু': 666,\n",
       " 'ে': 667,\n",
       " 'ো': 668,\n",
       " '্': 669,\n",
       " 'য়': 670,\n",
       " 'க': 671,\n",
       " 'த': 672,\n",
       " 'ப': 673,\n",
       " 'ம': 674,\n",
       " 'ய': 675,\n",
       " 'ர': 676,\n",
       " 'ல': 677,\n",
       " 'வ': 678,\n",
       " 'ா': 679,\n",
       " 'ி': 680,\n",
       " 'ு': 681,\n",
       " '்': 682,\n",
       " 'ร': 683,\n",
       " '་': 684,\n",
       " 'ག': 685,\n",
       " 'ང': 686,\n",
       " 'ད': 687,\n",
       " 'ན': 688,\n",
       " 'བ': 689,\n",
       " 'མ': 690,\n",
       " 'ར': 691,\n",
       " 'ལ': 692,\n",
       " 'ས': 693,\n",
       " 'ི': 694,\n",
       " 'ུ': 695,\n",
       " 'ེ': 696,\n",
       " 'ོ': 697,\n",
       " 'ა': 698,\n",
       " 'ე': 699,\n",
       " 'ი': 700,\n",
       " 'ლ': 701,\n",
       " 'ნ': 702,\n",
       " 'ო': 703,\n",
       " 'რ': 704,\n",
       " 'ს': 705,\n",
       " 'ᴬ': 706,\n",
       " 'ᴵ': 707,\n",
       " 'ᵀ': 708,\n",
       " 'ᵃ': 709,\n",
       " 'ᵇ': 710,\n",
       " 'ᵈ': 711,\n",
       " 'ᵉ': 712,\n",
       " 'ᵍ': 713,\n",
       " 'ᵏ': 714,\n",
       " 'ᵐ': 715,\n",
       " 'ᵒ': 716,\n",
       " 'ᵖ': 717,\n",
       " 'ᵗ': 718,\n",
       " 'ᵘ': 719,\n",
       " 'ᵢ': 720,\n",
       " 'ᵣ': 721,\n",
       " 'ᵤ': 722,\n",
       " 'ᵥ': 723,\n",
       " 'ᶜ': 724,\n",
       " 'ᶠ': 725,\n",
       " 'ḍ': 726,\n",
       " 'Ḥ': 727,\n",
       " 'ḥ': 728,\n",
       " 'Ḩ': 729,\n",
       " 'ḩ': 730,\n",
       " 'ḳ': 731,\n",
       " 'ṃ': 732,\n",
       " 'ṅ': 733,\n",
       " 'ṇ': 734,\n",
       " 'ṛ': 735,\n",
       " 'ṣ': 736,\n",
       " 'ṭ': 737,\n",
       " 'ạ': 738,\n",
       " 'ả': 739,\n",
       " 'ấ': 740,\n",
       " 'ầ': 741,\n",
       " 'ẩ': 742,\n",
       " 'ậ': 743,\n",
       " 'ắ': 744,\n",
       " 'ế': 745,\n",
       " 'ề': 746,\n",
       " 'ể': 747,\n",
       " 'ễ': 748,\n",
       " 'ệ': 749,\n",
       " 'ị': 750,\n",
       " 'ọ': 751,\n",
       " 'ố': 752,\n",
       " 'ồ': 753,\n",
       " 'ổ': 754,\n",
       " 'ộ': 755,\n",
       " 'ớ': 756,\n",
       " 'ờ': 757,\n",
       " 'ợ': 758,\n",
       " 'ụ': 759,\n",
       " 'ủ': 760,\n",
       " 'ứ': 761,\n",
       " 'ừ': 762,\n",
       " 'ử': 763,\n",
       " 'ữ': 764,\n",
       " 'ự': 765,\n",
       " 'ỳ': 766,\n",
       " 'ỹ': 767,\n",
       " 'ἀ': 768,\n",
       " 'ἐ': 769,\n",
       " 'ὁ': 770,\n",
       " 'ὐ': 771,\n",
       " 'ὰ': 772,\n",
       " 'ὶ': 773,\n",
       " 'ὸ': 774,\n",
       " 'ῆ': 775,\n",
       " 'ῖ': 776,\n",
       " 'ῦ': 777,\n",
       " 'ῶ': 778,\n",
       " '‐': 779,\n",
       " '‑': 780,\n",
       " '‒': 781,\n",
       " '–': 782,\n",
       " '—': 783,\n",
       " '―': 784,\n",
       " '‖': 785,\n",
       " '‘': 786,\n",
       " '’': 787,\n",
       " '‚': 788,\n",
       " '“': 789,\n",
       " '”': 790,\n",
       " '„': 791,\n",
       " '†': 792,\n",
       " '‡': 793,\n",
       " '•': 794,\n",
       " '…': 795,\n",
       " '‰': 796,\n",
       " '′': 797,\n",
       " '″': 798,\n",
       " '⁄': 799,\n",
       " '⁰': 800,\n",
       " 'ⁱ': 801,\n",
       " '⁴': 802,\n",
       " '⁵': 803,\n",
       " '⁶': 804,\n",
       " '⁷': 805,\n",
       " '⁸': 806,\n",
       " '⁹': 807,\n",
       " '⁺': 808,\n",
       " '⁻': 809,\n",
       " 'ⁿ': 810,\n",
       " '₀': 811,\n",
       " '₁': 812,\n",
       " '₂': 813,\n",
       " '₃': 814,\n",
       " '₄': 815,\n",
       " '₅': 816,\n",
       " '₆': 817,\n",
       " '₇': 818,\n",
       " '₈': 819,\n",
       " '₉': 820,\n",
       " '₊': 821,\n",
       " '₍': 822,\n",
       " '₎': 823,\n",
       " 'ₐ': 824,\n",
       " 'ₑ': 825,\n",
       " 'ₒ': 826,\n",
       " 'ₓ': 827,\n",
       " 'ₕ': 828,\n",
       " 'ₖ': 829,\n",
       " 'ₘ': 830,\n",
       " 'ₙ': 831,\n",
       " 'ₚ': 832,\n",
       " 'ₛ': 833,\n",
       " 'ₜ': 834,\n",
       " '₤': 835,\n",
       " '€': 836,\n",
       " '₱': 837,\n",
       " '₹': 838,\n",
       " 'ℓ': 839,\n",
       " '№': 840,\n",
       " 'ℝ': 841,\n",
       " '⅓': 842,\n",
       " '←': 843,\n",
       " '↑': 844,\n",
       " '→': 845,\n",
       " '↔': 846,\n",
       " '⇌': 847,\n",
       " '⇒': 848,\n",
       " '∂': 849,\n",
       " '∈': 850,\n",
       " '−': 851,\n",
       " '∗': 852,\n",
       " '∘': 853,\n",
       " '√': 854,\n",
       " '∞': 855,\n",
       " '∧': 856,\n",
       " '∨': 857,\n",
       " '∩': 858,\n",
       " '∪': 859,\n",
       " '≈': 860,\n",
       " '≠': 861,\n",
       " '≡': 862,\n",
       " '≤': 863,\n",
       " '≥': 864,\n",
       " '⊂': 865,\n",
       " '⊆': 866,\n",
       " '⊕': 867,\n",
       " '⋅': 868,\n",
       " '─': 869,\n",
       " '│': 870,\n",
       " '■': 871,\n",
       " '●': 872,\n",
       " '★': 873,\n",
       " '☆': 874,\n",
       " '☉': 875,\n",
       " '♠': 876,\n",
       " '♣': 877,\n",
       " '♥': 878,\n",
       " '♦': 879,\n",
       " '♭': 880,\n",
       " '♯': 881,\n",
       " '⟨': 882,\n",
       " '⟩': 883,\n",
       " 'ⱼ': 884,\n",
       " '、': 885,\n",
       " '。': 886,\n",
       " '《': 887,\n",
       " '》': 888,\n",
       " '「': 889,\n",
       " '」': 890,\n",
       " '『': 891,\n",
       " '』': 892,\n",
       " '〜': 893,\n",
       " 'い': 894,\n",
       " 'う': 895,\n",
       " 'え': 896,\n",
       " 'お': 897,\n",
       " 'か': 898,\n",
       " 'き': 899,\n",
       " 'く': 900,\n",
       " 'け': 901,\n",
       " 'こ': 902,\n",
       " 'さ': 903,\n",
       " 'し': 904,\n",
       " 'す': 905,\n",
       " 'せ': 906,\n",
       " 'そ': 907,\n",
       " 'た': 908,\n",
       " 'ち': 909,\n",
       " 'つ': 910,\n",
       " 'て': 911,\n",
       " 'と': 912,\n",
       " 'な': 913,\n",
       " 'に': 914,\n",
       " 'の': 915,\n",
       " 'は': 916,\n",
       " 'ひ': 917,\n",
       " 'ま': 918,\n",
       " 'み': 919,\n",
       " 'む': 920,\n",
       " 'め': 921,\n",
       " 'も': 922,\n",
       " 'や': 923,\n",
       " 'ゆ': 924,\n",
       " 'よ': 925,\n",
       " 'ら': 926,\n",
       " 'り': 927,\n",
       " 'る': 928,\n",
       " 'れ': 929,\n",
       " 'ん': 930,\n",
       " 'ア': 931,\n",
       " 'ィ': 932,\n",
       " 'イ': 933,\n",
       " 'ウ': 934,\n",
       " 'エ': 935,\n",
       " 'オ': 936,\n",
       " 'カ': 937,\n",
       " 'ガ': 938,\n",
       " 'キ': 939,\n",
       " 'ク': 940,\n",
       " 'グ': 941,\n",
       " 'コ': 942,\n",
       " 'サ': 943,\n",
       " 'シ': 944,\n",
       " 'ジ': 945,\n",
       " 'ス': 946,\n",
       " 'ズ': 947,\n",
       " 'タ': 948,\n",
       " 'ダ': 949,\n",
       " 'ッ': 950,\n",
       " 'テ': 951,\n",
       " 'デ': 952,\n",
       " 'ト': 953,\n",
       " 'ド': 954,\n",
       " 'ナ': 955,\n",
       " 'ニ': 956,\n",
       " 'ハ': 957,\n",
       " 'バ': 958,\n",
       " 'パ': 959,\n",
       " 'フ': 960,\n",
       " 'ブ': 961,\n",
       " 'プ': 962,\n",
       " 'マ': 963,\n",
       " 'ミ': 964,\n",
       " 'ム': 965,\n",
       " 'ャ': 966,\n",
       " 'ュ': 967,\n",
       " 'ラ': 968,\n",
       " 'リ': 969,\n",
       " 'ル': 970,\n",
       " 'レ': 971,\n",
       " 'ロ': 972,\n",
       " 'ン': 973,\n",
       " '・': 974,\n",
       " 'ー': 975,\n",
       " '一': 976,\n",
       " '三': 977,\n",
       " '上': 978,\n",
       " '下': 979,\n",
       " '中': 980,\n",
       " '事': 981,\n",
       " '二': 982,\n",
       " '井': 983,\n",
       " '京': 984,\n",
       " '人': 985,\n",
       " '亻': 986,\n",
       " '仁': 987,\n",
       " '佐': 988,\n",
       " '侍': 989,\n",
       " '光': 990,\n",
       " '公': 991,\n",
       " '力': 992,\n",
       " '北': 993,\n",
       " '十': 994,\n",
       " '南': 995,\n",
       " '原': 996,\n",
       " '口': 997,\n",
       " '史': 998,\n",
       " '司': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "wordpiece_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(len(wordpiece_tokenizer.get_vocab()))\n",
    "wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Special', 'cases', 'aren', \"'\", 't', 'special', 'enough', 'to', 'break', 'the', 'rules', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpiece_tokenizer.tokenize(python_zen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word', 'piece</w>']\n",
      "['word', '##piece']\n"
     ]
    }
   ],
   "source": [
    "print(bpe_tokenizer.tokenize('wordpiece'))\n",
    "print(wordpiece_tokenizer.tokenize('wordpiece'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m.</w>', 'night</w>', 'shy', 'am', 'alan</w>']\n",
      "['M', '.', 'Night', 'S', '##hya', '##mal', '##an']\n"
     ]
    }
   ],
   "source": [
    "# Like directors\n",
    "print(bpe_tokenizer.tokenize('M. Night Shyamalan'))\n",
    "print(wordpiece_tokenizer.tokenize('M. Night Shyamalan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':</w>', 'hippo', 'po', 'tom', 'on', 'stro', 'se', 'squi', 'pp', 'ed', 'ali', 'op', 'ho', 'bia</w>']\n",
      "[':', 'Hip', '##pop', '##oto', '##mons', '##tro', '##ses', '##qui', '##pped', '##ali', '##op', '##ho', '##bia']\n"
     ]
    }
   ],
   "source": [
    "# The fear of long words\n",
    "print(bpe_tokenizer.tokenize(\":Hippopotomonstrosesquippedaliophobia\"))\n",
    "print(wordpiece_tokenizer.tokenize(\":Hippopotomonstrosesquippedaliophobia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['projec', 't', 'contrac', 't', 'char', 'ging', 'peri', 'od', 'projec', 't', 'accoun', 'tre', 'fer', 'en', 'c', 'ev', 'm</w>']\n",
      "['Project', '##C', '##ont', '##rac', '##t', '##C', '##har', '##ging', '##P', '##eri', '##od', '##P', '##ro', '##ject', '##A', '##cco', '##unt', '##R', '##ef', '##ere', '##nce', '##V', '##M']\n"
     ]
    }
   ],
   "source": [
    "# A very long Java Class name\n",
    "print(bpe_tokenizer.tokenize('ProjectContractChargingPeriodProjectAccountReferenceVM'))  \n",
    "print(wordpiece_tokenizer.tokenize('ProjectContractChargingPeriodProjectAccountReferenceVM'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unigram\n",
    "- Takes a opposite approach as BPE and WordPiece: starts with many long words and breaks them up. \n",
    "- Tries to remove words with the least impact on the loss (i.e. are least required). \n",
    "- Unigram is often used through SentencePiece\n",
    "- Used in SOTA models such as: AlBERT, T5, mBART, Big Bird, and XLNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take the same corpus again:\n",
    "```yaml\n",
    "corpus: (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "vocabulary: [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"]\n",
    "```\n",
    "The vocab is all possible subtokens, that aren't spanning the whole word. \n",
    "\n",
    "We see `hug` is in here, due to `hugs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The unigram model sees each token independently of any token that came prior. The probability of each token given the previous context is just the probability of that token. \n",
    "The probability of a token is calculated over the total vocabulary:\n",
    "$$ \\mathcal{P}(t) = \\frac{f_t}{\\sum_{t \\in T}f_t} $$\n",
    "\n",
    "Where $T$ is the set of tokens and $f_t$ indicates the frequency of a token. \n",
    "\n",
    "So pretty simple: if the token `ug` is present 20 times, and the sum of all token frequencies is 210, we get: \n",
    "\n",
    "$$ \\mathcal{P}(\\text{\"ug\"}) = \\frac{20}{210} = 0.095 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```yaml\n",
    "corpus: (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "vocab: (\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16)\n",
    "(\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5)\n",
    "```\n",
    "\n",
    "These are the frequencies for each token in our vocabulary. The frequency sum is 210. \n",
    "\n",
    "If we want to calculate the probability of a word, we just multiple the probabilities of each token. \n",
    "\n",
    "$$ \\mathcal{P}([\\text{\"p\"}, \\text{\"u\"}, \\text{\"g\"}]) = \\mathcal{P}(\\text{\"p\"}) \\times \\mathcal{P}(\\text{\"u\"}) \\times \\mathcal{P}(\\text{\"g\"}) = \\frac{17}{210} \\times \\frac{36}{210} \\times \\frac{20}{210} = 0.0013 $$\n",
    "\n",
    "For each word, the chosen representation is the one with the highest probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we train then? \n",
    "The model is essentially overfitted to the dataset. \n",
    "1. Calculate the loss for the whole corpus, with the current vocabulary. \n",
    "2. Remove the vocabulary token whose removal increases the loss the least. \n",
    "3. We continue removing tokens until we reach our desired vocabulary size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Loss for one word is calculated as:\n",
    "\n",
    "$$ \\text{loss}_w = f_w \\cdot -\\log \\mathcal{P}(w)  $$\n",
    "So taking `pug` as an example:\n",
    "$$\n",
    " \\text{loss}_{\\text{\"pug\"}} = 5 \\cdot -\\log0.0013 \\\\ \n",
    " = 5 \\cdot 2.88 \\\\\n",
    " = 14.43  $$\n",
    " \n",
    " Total loss is then calculated with:\n",
    " $$\\text{loss} = \\sum_{w \\in W} \\text{loss}_w $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algorithm is as follows:\n",
    "1. Calculate loss for each word in the corpus with current vocab and sum.\n",
    "2. Iteratively remove a token from the vocabulary.\n",
    "3. Calculate loss with new vocabulary.\n",
    "4. Remove the token with the lowest negative effect on the loss.\n",
    "5. If vocab size > desired vocab size: go back to step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# And that's how you train a unigram tokenizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In conclusion\n",
    "\n",
    "3 types of tokenizers:\n",
    "- Wordbased\n",
    "- Character-based\n",
    "- Subword-based \n",
    "\n",
    "Where Subword tokenization has three different methods:\n",
    "- BPE - Pair frequency based\n",
    "- WordPiece/SentencePiece - Pair frequency vs token frequency based\n",
    "- Unigram - Loss minimization"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "c38f211982bd679d5ca5d79a71d7517c654455bae4af5c30a3c557803a2cce89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
